nohup: ignoring input
Deepfashion Consumer-to-shop Attribute prediction using single model
arch           is: Resnet152
gpuId          is: 1
init lr        is: 0.001
num_classs     is: 257
batch size     is: 64
input_size     is: 224
relative_path  is: ../Img/
train_path     is: ../NewAnno/list_bbx_image_attr_train.txt
val_path       is: ../NewAnno/list_bbx_image_attr_val.txt
test_path      is: ../NewAnno/list_bbx_image_attr_test.txt
epochs         is: 50
savePath       is: ./checkpoint/BBX/NewAnnoCleaned
resume         is: 
momentum       is: 0.9
zeroTrain      is: False
weight_decay   is: 0.0005
print_freq     is: 50
Conv_Type      is: 2
Branch         is: 17
Conv_Num       is: 5
Use GPU!
Start training.
('Epoch: ', 0, 'lr is: [0.001]')
        the 0th batch, loss is: 1.19982612   
       the 50th batch, loss is: 0.48570153   
      the 100th batch, loss is: 0.50484085   
      the 150th batch, loss is: 0.44349223   
      the 200th batch, loss is: 0.45261979   
      the 250th batch, loss is: 0.45625845   
      the 300th batch, loss is: 0.47584724   
      the 350th batch, loss is: 0.37364149   
      the 400th batch, loss is: 0.39261082   
      the 450th batch, loss is: 0.43871796   
      the 500th batch, loss is: 0.35875562   
      the 550th batch, loss is: 0.38898972   
      the 600th batch, loss is: 0.37436792   
      the 650th batch, loss is: 0.41433015   
      the 700th batch, loss is: 0.44149256   
      the 750th batch, loss is: 0.40713453   
      the 800th batch, loss is: 0.39399919   
      the 850th batch, loss is: 0.40560248   
      the 900th batch, loss is: 0.40796202   
      the 950th batch, loss is: 0.37891009   
     the 1000th batch, loss is: 0.42134324   
     the 1050th batch, loss is: 0.36783826   
     the 1100th batch, loss is: 0.41923451   
     the 1150th batch, loss is: 0.37522423   
     the 1200th batch, loss is: 0.34385249   
     the 1250th batch, loss is: 0.40683696   
     the 1300th batch, loss is: 0.40687233   
     the 1350th batch, loss is: 0.37760308   
     the 1400th batch, loss is: 0.42758575   
     the 1450th batch, loss is: 0.35236579   
     the 1500th batch, loss is: 0.42235348   
     the 1550th batch, loss is: 0.38629001   
     the 1600th batch, loss is: 0.38151067   
     the 1650th batch, loss is: 0.34136984   
     the 1700th batch, loss is: 0.36061558   
     the 1750th batch, loss is: 0.38602632   
     the 1800th batch, loss is: 0.42291534   
     the 1850th batch, loss is: 0.37475404   
     the 1900th batch, loss is: 0.37278384   
     the 1950th batch, loss is: 0.38814873   
     the 2000th batch, loss is: 0.33694461   
     the 2050th batch, loss is: 0.38100475   
     the 2100th batch, loss is: 0.34293488   
     the 2150th batch, loss is: 0.40398681   
     the 2200th batch, loss is: 0.35659152   
     the 2250th batch, loss is: 0.39360809   
     the 2300th batch, loss is: 0.3454051    
     the 2350th batch, loss is: 0.33863121   
     the 2400th batch, loss is: 0.37228981   
     the 2450th batch, loss is: 0.37224957   
     the 2500th batch, loss is: 0.36201909   
     the 2550th batch, loss is: 0.41125882   
     the 2600th batch, loss is: 0.36641905   
     the 2650th batch, loss is: 0.36132923   
     the 2700th batch, loss is: 0.36086777   
     the 2750th batch, loss is: 0.38652539   
     the 2800th batch, loss is: 0.34608513   
     the 2850th batch, loss is: 0.38392836   
     the 2900th batch, loss is: 0.39268839   
     the 2950th batch, loss is: 0.39585954   
('Epoch: ', 1, 'lr is: [0.001]')
     the 3000th batch, loss is: 0.38083249   
     the 3050th batch, loss is: 0.35469824   
     the 3100th batch, loss is: 0.34752887   
     the 3150th batch, loss is: 0.42551479   
     the 3200th batch, loss is: 0.3404637    
     the 3250th batch, loss is: 0.33490372   
     the 3300th batch, loss is: 0.37096739   
     the 3350th batch, loss is: 0.3481198    
     the 3400th batch, loss is: 0.38349184   
     the 3450th batch, loss is: 0.34810749   
     the 3500th batch, loss is: 0.30328205   
     the 3550th batch, loss is: 0.30867746   
     the 3600th batch, loss is: 0.32677448   
     the 3650th batch, loss is: 0.36268738   
     the 3700th batch, loss is: 0.29378003   
     the 3750th batch, loss is: 0.34872445   
     the 3800th batch, loss is: 0.35330242   
     the 3850th batch, loss is: 0.34709507   
     the 3900th batch, loss is: 0.32602444   
     the 3950th batch, loss is: 0.31973788   
     the 4000th batch, loss is: 0.31380555   
     the 4050th batch, loss is: 0.33033204   
     the 4100th batch, loss is: 0.36396235   
     the 4150th batch, loss is: 0.33743694   
     the 4200th batch, loss is: 0.28523514   
     the 4250th batch, loss is: 0.3375611    
     the 4300th batch, loss is: 0.33297428   
     the 4350th batch, loss is: 0.30842355   
     the 4400th batch, loss is: 0.31592837   
     the 4450th batch, loss is: 0.34550521   
     the 4500th batch, loss is: 0.33870241   
     the 4550th batch, loss is: 0.33973226   
     the 4600th batch, loss is: 0.38876966   
     the 4650th batch, loss is: 0.38799173   
     the 4700th batch, loss is: 0.34527123   
     the 4750th batch, loss is: 0.30821854   
     the 4800th batch, loss is: 0.3096948    
     the 4850th batch, loss is: 0.3024053    
     the 4900th batch, loss is: 0.34274852   
     the 4950th batch, loss is: 0.33167553   
     the 5000th batch, loss is: 0.30073899   
     the 5050th batch, loss is: 0.34656909   
     the 5100th batch, loss is: 0.33651292   
     the 5150th batch, loss is: 0.36469093   
     the 5200th batch, loss is: 0.34688997   
     the 5250th batch, loss is: 0.33829162   
     the 5300th batch, loss is: 0.35296795   
     the 5350th batch, loss is: 0.33957151   
     the 5400th batch, loss is: 0.34746534   
     the 5450th batch, loss is: 0.35745114   
     the 5500th batch, loss is: 0.38944805   
     the 5550th batch, loss is: 0.30796576   
     the 5600th batch, loss is: 0.3606405    
     the 5650th batch, loss is: 0.3261123    
     the 5700th batch, loss is: 0.35814297   
     the 5750th batch, loss is: 0.38389021   
     the 5800th batch, loss is: 0.32922941   
     the 5850th batch, loss is: 0.33519629   
     the 5900th batch, loss is: 0.32721382   
     the 5950th batch, loss is: 0.32651639   
('Epoch: ', 2, 'lr is: [0.001]')
     the 6000th batch, loss is: 0.29737145   
     the 6050th batch, loss is: 0.29965913   
     the 6100th batch, loss is: 0.31900284   
     the 6150th batch, loss is: 0.33695436   
     the 6200th batch, loss is: 0.35443491   
     the 6250th batch, loss is: 0.30607036   
     the 6300th batch, loss is: 0.31888124   
     the 6350th batch, loss is: 0.3213999    
     the 6400th batch, loss is: 0.30253547   
     the 6450th batch, loss is: 0.33967265   
     the 6500th batch, loss is: 0.30618024   
     the 6550th batch, loss is: 0.29369023   
     the 6600th batch, loss is: 0.27852872   
     the 6650th batch, loss is: 0.31803665   
     the 6700th batch, loss is: 0.30807969   
     the 6750th batch, loss is: 0.2897085    
     the 6800th batch, loss is: 0.28588787   
     the 6850th batch, loss is: 0.29549861   
     the 6900th batch, loss is: 0.33022356   
     the 6950th batch, loss is: 0.29212591   
     the 7000th batch, loss is: 0.29227871   
     the 7050th batch, loss is: 0.26852992   
     the 7100th batch, loss is: 0.34946716   
     the 7150th batch, loss is: 0.34400758   
     the 7200th batch, loss is: 0.29362333   
     the 7250th batch, loss is: 0.34318843   
     the 7300th batch, loss is: 0.33503354   
     the 7350th batch, loss is: 0.29611436   
     the 7400th batch, loss is: 0.29209051   
     the 7450th batch, loss is: 0.34607095   
     the 7500th batch, loss is: 0.35129616   
     the 7550th batch, loss is: 0.33982047   
     the 7600th batch, loss is: 0.3173545    
     the 7650th batch, loss is: 0.2904447    
     the 7700th batch, loss is: 0.32766435   
     the 7750th batch, loss is: 0.36253351   
     the 7800th batch, loss is: 0.35191363   
     the 7850th batch, loss is: 0.3132166    
     the 7900th batch, loss is: 0.31094348   
     the 7950th batch, loss is: 0.31671754   
     the 8000th batch, loss is: 0.33397201   
     the 8050th batch, loss is: 0.28261629   
     the 8100th batch, loss is: 0.34126735   
     the 8150th batch, loss is: 0.26570261   
     the 8200th batch, loss is: 0.32459742   
     the 8250th batch, loss is: 0.3658815    
     the 8300th batch, loss is: 0.3155987    
     the 8350th batch, loss is: 0.31606841   
     the 8400th batch, loss is: 0.3000665    
     the 8450th batch, loss is: 0.32416505   
     the 8500th batch, loss is: 0.30969697   
     the 8550th batch, loss is: 0.32964167   
     the 8600th batch, loss is: 0.29211414   
     the 8650th batch, loss is: 0.35074005   
     the 8700th batch, loss is: 0.33142754   
     the 8750th batch, loss is: 0.33143008   
     the 8800th batch, loss is: 0.30596828   
     the 8850th batch, loss is: 0.34866858   
     the 8900th batch, loss is: 0.34226561   
     the 8950th batch, loss is: 0.32568842   
('Epoch: ', 3, 'lr is: [0.001]')
     the 9000th batch, loss is: 0.300789     
     the 9050th batch, loss is: 0.31772351   
     the 9100th batch, loss is: 0.30785468   
     the 9150th batch, loss is: 0.30908051   
     the 9200th batch, loss is: 0.27373004   
     the 9250th batch, loss is: 0.28949133   
     the 9300th batch, loss is: 0.30292597   
     the 9350th batch, loss is: 0.32938263   
     the 9400th batch, loss is: 0.29993752   
     the 9450th batch, loss is: 0.27887136   
     the 9500th batch, loss is: 0.31320289   
     the 9550th batch, loss is: 0.30784953   
     the 9600th batch, loss is: 0.24388859   
     the 9650th batch, loss is: 0.2773121    
     the 9700th batch, loss is: 0.28081381   
     the 9750th batch, loss is: 0.31218883   
     the 9800th batch, loss is: 0.25517312   
     the 9850th batch, loss is: 0.26321989   
     the 9900th batch, loss is: 0.30883577   
     the 9950th batch, loss is: 0.33471298   
    the 10000th batch, loss is: 0.29410675   
    the 10050th batch, loss is: 0.31670082   
    the 10100th batch, loss is: 0.28281701   
    the 10150th batch, loss is: 0.27447963   
    the 10200th batch, loss is: 0.29491514   
    the 10250th batch, loss is: 0.26823485   
    the 10300th batch, loss is: 0.28926149   
    the 10350th batch, loss is: 0.26888821   
    the 10400th batch, loss is: 0.27311674   
    the 10450th batch, loss is: 0.32333168   
    the 10500th batch, loss is: 0.32359436   
    the 10550th batch, loss is: 0.31386027   
    the 10600th batch, loss is: 0.27248865   
    the 10650th batch, loss is: 0.32983628   
    the 10700th batch, loss is: 0.25988695   
    the 10750th batch, loss is: 0.30985177   
    the 10800th batch, loss is: 0.29253754   
    the 10850th batch, loss is: 0.28361186   
    the 10900th batch, loss is: 0.32810366   
    the 10950th batch, loss is: 0.36076093   
    the 11000th batch, loss is: 0.32768288   
    the 11050th batch, loss is: 0.30410716   
    the 11100th batch, loss is: 0.28704777   
    the 11150th batch, loss is: 0.29588696   
    the 11200th batch, loss is: 0.30258375   
    the 11250th batch, loss is: 0.25886884   
    the 11300th batch, loss is: 0.27406213   
    the 11350th batch, loss is: 0.27081114   
    the 11400th batch, loss is: 0.28824604   
    the 11450th batch, loss is: 0.29008919   
    the 11500th batch, loss is: 0.27606943   
    the 11550th batch, loss is: 0.33177343   
    the 11600th batch, loss is: 0.315577     
    the 11650th batch, loss is: 0.31917134   
    the 11700th batch, loss is: 0.27452829   
    the 11750th batch, loss is: 0.34707186   
    the 11800th batch, loss is: 0.32904711   
    the 11850th batch, loss is: 0.3113969    
    the 11900th batch, loss is: 0.29140082   
    the 11950th batch, loss is: 0.28339061   
('Epoch: ', 4, 'lr is: [0.001]')
    the 12000th batch, loss is: 0.27824095   
    the 12050th batch, loss is: 0.31427723   
    the 12100th batch, loss is: 0.27657872   
    the 12150th batch, loss is: 0.30205551   
    the 12200th batch, loss is: 0.28027958   
    the 12250th batch, loss is: 0.27529389   
    the 12300th batch, loss is: 0.24113829   
    the 12350th batch, loss is: 0.2990213    
    the 12400th batch, loss is: 0.27934363   
    the 12450th batch, loss is: 0.27811649   
    the 12500th batch, loss is: 0.2853539    
    the 12550th batch, loss is: 0.2629652    
    the 12600th batch, loss is: 0.30882958   
    the 12650th batch, loss is: 0.25339162   
    the 12700th batch, loss is: 0.27568623   
    the 12750th batch, loss is: 0.29828727   
    the 12800th batch, loss is: 0.27852818   
    the 12850th batch, loss is: 0.3084833    
    the 12900th batch, loss is: 0.28689265   
    the 12950th batch, loss is: 0.23359719   
    the 13000th batch, loss is: 0.27901745   
    the 13050th batch, loss is: 0.27580228   
    the 13100th batch, loss is: 0.24795365   
    the 13150th batch, loss is: 0.2903685    
    the 13200th batch, loss is: 0.26334253   
    the 13250th batch, loss is: 0.31035459   
    the 13300th batch, loss is: 0.28024575   
    the 13350th batch, loss is: 0.26759416   
    the 13400th batch, loss is: 0.26932755   
    the 13450th batch, loss is: 0.27430341   
    the 13500th batch, loss is: 0.31739545   
    the 13550th batch, loss is: 0.2498872    
    the 13600th batch, loss is: 0.27322048   
    the 13650th batch, loss is: 0.2912398    
    the 13700th batch, loss is: 0.27533805   
    the 13750th batch, loss is: 0.24976149   
    the 13800th batch, loss is: 0.25707424   
    the 13850th batch, loss is: 0.26712728   
    the 13900th batch, loss is: 0.2734248    
    the 13950th batch, loss is: 0.26440579   
    the 14000th batch, loss is: 0.28539279   
    the 14050th batch, loss is: 0.3618871    
    the 14100th batch, loss is: 0.25294575   
    the 14150th batch, loss is: 0.25776258   
    the 14200th batch, loss is: 0.2730355    
    the 14250th batch, loss is: 0.29385021   
    the 14300th batch, loss is: 0.28334984   
    the 14350th batch, loss is: 0.28143689   
    the 14400th batch, loss is: 0.29788262   
    the 14450th batch, loss is: 0.25558132   
    the 14500th batch, loss is: 0.28198233   
    the 14550th batch, loss is: 0.29346925   
    the 14600th batch, loss is: 0.30858427   
    the 14650th batch, loss is: 0.26818421   
    the 14700th batch, loss is: 0.29043058   
    the 14750th batch, loss is: 0.27476382   
    the 14800th batch, loss is: 0.27242059   
    the 14850th batch, loss is: 0.2570926    
    the 14900th batch, loss is: 0.30393428   
    the 14950th batch, loss is: 0.30352288   
('Epoch: ', 5, 'lr is: [0.001]')
    the 15000th batch, loss is: 0.26595828   
    the 15050th batch, loss is: 0.25569603   
    the 15100th batch, loss is: 0.26529092   
    the 15150th batch, loss is: 0.28914195   
    the 15200th batch, loss is: 0.29305938   
    the 15250th batch, loss is: 0.24167998   
    the 15300th batch, loss is: 0.25114214   
    the 15350th batch, loss is: 0.25553676   
    the 15400th batch, loss is: 0.21848369   
    the 15450th batch, loss is: 0.26322219   
    the 15500th batch, loss is: 0.28217056   
    the 15550th batch, loss is: 0.2715795    
    the 15600th batch, loss is: 0.23165479   
    the 15650th batch, loss is: 0.25710371   
    the 15700th batch, loss is: 0.26964894   
    the 15750th batch, loss is: 0.23511626   
    the 15800th batch, loss is: 0.25146252   
    the 15850th batch, loss is: 0.25460088   
    the 15900th batch, loss is: 0.27398807   
    the 15950th batch, loss is: 0.29669526   
    the 16000th batch, loss is: 0.23943147   
    the 16050th batch, loss is: 0.23148975   
    the 16100th batch, loss is: 0.21350291   
    the 16150th batch, loss is: 0.26990813   
    the 16200th batch, loss is: 0.24329129   
    the 16250th batch, loss is: 0.26131171   
    the 16300th batch, loss is: 0.27776816   
    the 16350th batch, loss is: 0.24692059   
    the 16400th batch, loss is: 0.27802911   
    the 16450th batch, loss is: 0.25789323   
    the 16500th batch, loss is: 0.29060152   
    the 16550th batch, loss is: 0.22495112   
    the 16600th batch, loss is: 0.25951841   
    the 16650th batch, loss is: 0.26917273   
    the 16700th batch, loss is: 0.27485338   
    the 16750th batch, loss is: 0.26560366   
    the 16800th batch, loss is: 0.23690121   
    the 16850th batch, loss is: 0.25406712   
    the 16900th batch, loss is: 0.24258798   
    the 16950th batch, loss is: 0.23761483   
    the 17000th batch, loss is: 0.28504106   
    the 17050th batch, loss is: 0.28296039   
    the 17100th batch, loss is: 0.2532787    
    the 17150th batch, loss is: 0.31593305   
    the 17200th batch, loss is: 0.25572801   
    the 17250th batch, loss is: 0.26351199   
    the 17300th batch, loss is: 0.28762648   
    the 17350th batch, loss is: 0.26804501   
    the 17400th batch, loss is: 0.27223703   
    the 17450th batch, loss is: 0.29549399   
    the 17500th batch, loss is: 0.26474985   
    the 17550th batch, loss is: 0.20781997   
    the 17600th batch, loss is: 0.24598528   
    the 17650th batch, loss is: 0.29808789   
    the 17700th batch, loss is: 0.26379251   
    the 17750th batch, loss is: 0.29793495   
    the 17800th batch, loss is: 0.2619012    
    the 17850th batch, loss is: 0.2665062    
    the 17900th batch, loss is: 0.30434704   
    the 17950th batch, loss is: 0.25569648   
('Epoch: ', 6, 'lr is: [0.001]')
    the 18000th batch, loss is: 0.22332323   
    the 18050th batch, loss is: 0.20529629   
    the 18100th batch, loss is: 0.22660668   
    the 18150th batch, loss is: 0.20300767   
    the 18200th batch, loss is: 0.25452656   
    the 18250th batch, loss is: 0.22471276   
    the 18300th batch, loss is: 0.20666699   
    the 18350th batch, loss is: 0.22227259   
    the 18400th batch, loss is: 0.20477976   
    the 18450th batch, loss is: 0.22474357   
    the 18500th batch, loss is: 0.24189657   
    the 18550th batch, loss is: 0.26226786   
    the 18600th batch, loss is: 0.22609119   
    the 18650th batch, loss is: 0.27409247   
    the 18700th batch, loss is: 0.22419895   
    the 18750th batch, loss is: 0.24388836   
    the 18800th batch, loss is: 0.23001009   
    the 18850th batch, loss is: 0.22701259   
    the 18900th batch, loss is: 0.24237934   
    the 18950th batch, loss is: 0.24060865   
    the 19000th batch, loss is: 0.27059287   
    the 19050th batch, loss is: 0.24613284   
    the 19100th batch, loss is: 0.23070306   
    the 19150th batch, loss is: 0.2153237    
    the 19200th batch, loss is: 0.2906473    
    the 19250th batch, loss is: 0.26158139   
    the 19300th batch, loss is: 0.23234662   
    the 19350th batch, loss is: 0.25903627   
    the 19400th batch, loss is: 0.26650855   
    the 19450th batch, loss is: 0.24699636   
    the 19500th batch, loss is: 0.27526563   
    the 19550th batch, loss is: 0.25002733   
    the 19600th batch, loss is: 0.27972025   
    the 19650th batch, loss is: 0.19805041   
    the 19700th batch, loss is: 0.33595195   
    the 19750th batch, loss is: 0.25114003   
    the 19800th batch, loss is: 0.24262062   
    the 19850th batch, loss is: 0.23063183   
    the 19900th batch, loss is: 0.25220686   
    the 19950th batch, loss is: 0.22806801   
    the 20000th batch, loss is: 0.23546599   
    the 20050th batch, loss is: 0.26739141   
    the 20100th batch, loss is: 0.24775536   
    the 20150th batch, loss is: 0.21462972   
    the 20200th batch, loss is: 0.22221078   
    the 20250th batch, loss is: 0.22629455   
    the 20300th batch, loss is: 0.23876844   
    the 20350th batch, loss is: 0.23135896   
    the 20400th batch, loss is: 0.22905333   
    the 20450th batch, loss is: 0.22779812   
    the 20500th batch, loss is: 0.24060051   
    the 20550th batch, loss is: 0.22246456   
    the 20600th batch, loss is: 0.25410786   
    the 20650th batch, loss is: 0.30528554   
    the 20700th batch, loss is: 0.25772858   
    the 20750th batch, loss is: 0.27097395   
    the 20800th batch, loss is: 0.27224997   
    the 20850th batch, loss is: 0.29528695   
    the 20900th batch, loss is: 0.25294614   
    the 20950th batch, loss is: 0.24020635   
('Epoch: ', 7, 'lr is: [0.001]')
    the 21000th batch, loss is: 0.18442386   
    the 21050th batch, loss is: 0.19394356   
    the 21100th batch, loss is: 0.19349778   
    the 21150th batch, loss is: 0.19645451   
    the 21200th batch, loss is: 0.2352924    
    the 21250th batch, loss is: 0.22162364   
    the 21300th batch, loss is: 0.22168063   
    the 21350th batch, loss is: 0.19709125   
    the 21400th batch, loss is: 0.19392219   
    the 21450th batch, loss is: 0.22075365   
    the 21500th batch, loss is: 0.22464049   
    the 21550th batch, loss is: 0.22931285   
    the 21600th batch, loss is: 0.19494411   
    the 21650th batch, loss is: 0.21998796   
    the 21700th batch, loss is: 0.22488052   
    the 21750th batch, loss is: 0.23318245   
    the 21800th batch, loss is: 0.18562572   
    the 21850th batch, loss is: 0.22755946   
    the 21900th batch, loss is: 0.17148806   
    the 21950th batch, loss is: 0.2401409    
    the 22000th batch, loss is: 0.22947405   
    the 22050th batch, loss is: 0.19982824   
    the 22100th batch, loss is: 0.24136744   
    the 22150th batch, loss is: 0.28573912   
    the 22200th batch, loss is: 0.19206133   
    the 22250th batch, loss is: 0.20126048   
    the 22300th batch, loss is: 0.21221462   
    the 22350th batch, loss is: 0.21626858   
    the 22400th batch, loss is: 0.18608804   
    the 22450th batch, loss is: 0.24631757   
    the 22500th batch, loss is: 0.20766744   
    the 22550th batch, loss is: 0.19896737   
    the 22600th batch, loss is: 0.21680959   
    the 22650th batch, loss is: 0.22087128   
    the 22700th batch, loss is: 0.20591791   
    the 22750th batch, loss is: 0.21266352   
    the 22800th batch, loss is: 0.22060961   
    the 22850th batch, loss is: 0.21316345   
    the 22900th batch, loss is: 0.20081812   
    the 22950th batch, loss is: 0.25195232   
    the 23000th batch, loss is: 0.22042224   
    the 23050th batch, loss is: 0.22916642   
    the 23100th batch, loss is: 0.20572725   
    the 23150th batch, loss is: 0.19553666   
    the 23200th batch, loss is: 0.17989787   
    the 23250th batch, loss is: 0.22308002   
    the 23300th batch, loss is: 0.24216729   
    the 23350th batch, loss is: 0.2450989    
    the 23400th batch, loss is: 0.23719186   
    the 23450th batch, loss is: 0.22058739   
    the 23500th batch, loss is: 0.2021686    
    the 23550th batch, loss is: 0.20343906   
    the 23600th batch, loss is: 0.23496576   
    the 23650th batch, loss is: 0.20606019   
    the 23700th batch, loss is: 0.19612128   
    the 23750th batch, loss is: 0.23042919   
    the 23800th batch, loss is: 0.22802293   
    the 23850th batch, loss is: 0.17509323   
    the 23900th batch, loss is: 0.23506679   
    the 23950th batch, loss is: 0.22799686   
('Epoch: ', 8, 'lr is: [0.001]')
    the 24000th batch, loss is: 0.15592346   
    the 24050th batch, loss is: 0.1986815    
    the 24100th batch, loss is: 0.17197703   
    the 24150th batch, loss is: 0.17942159   
    the 24200th batch, loss is: 0.19181056   
    the 24250th batch, loss is: 0.2025194    
    the 24300th batch, loss is: 0.20281798   
    the 24350th batch, loss is: 0.20861293   
    the 24400th batch, loss is: 0.20963061   
    the 24450th batch, loss is: 0.17550164   
    the 24500th batch, loss is: 0.17306176   
    the 24550th batch, loss is: 0.21924199   
    the 24600th batch, loss is: 0.20198977   
    the 24650th batch, loss is: 0.22516336   
    the 24700th batch, loss is: 0.17695467   
    the 24750th batch, loss is: 0.18150097   
    the 24800th batch, loss is: 0.19882514   
    the 24850th batch, loss is: 0.1928052    
    the 24900th batch, loss is: 0.19917502   
    the 24950th batch, loss is: 0.20353462   
    the 25000th batch, loss is: 0.21630634   
    the 25050th batch, loss is: 0.17926601   
    the 25100th batch, loss is: 0.20606793   
    the 25150th batch, loss is: 0.20716056   
    the 25200th batch, loss is: 0.19908725   
    the 25250th batch, loss is: 0.19784462   
    the 25300th batch, loss is: 0.16559049   
    the 25350th batch, loss is: 0.22176446   
    the 25400th batch, loss is: 0.2139557    
    the 25450th batch, loss is: 0.17854437   
    the 25500th batch, loss is: 0.18767238   
    the 25550th batch, loss is: 0.17961442   
    the 25600th batch, loss is: 0.22461106   
    the 25650th batch, loss is: 0.15982866   
    the 25700th batch, loss is: 0.23120898   
    the 25750th batch, loss is: 0.2294586    
    the 25800th batch, loss is: 0.19310717   
    the 25850th batch, loss is: 0.19721495   
    the 25900th batch, loss is: 0.21902898   
    the 25950th batch, loss is: 0.22480708   
    the 26000th batch, loss is: 0.18786657   
    the 26050th batch, loss is: 0.19014987   
    the 26100th batch, loss is: 0.18786526   
    the 26150th batch, loss is: 0.21110851   
    the 26200th batch, loss is: 0.20705271   
    the 26250th batch, loss is: 0.23632385   
    the 26300th batch, loss is: 0.2191063    
    the 26350th batch, loss is: 0.20354992   
    the 26400th batch, loss is: 0.18723384   
    the 26450th batch, loss is: 0.19815923   
    the 26500th batch, loss is: 0.1838617    
    the 26550th batch, loss is: 0.21684277   
    the 26600th batch, loss is: 0.21497679   
    the 26650th batch, loss is: 0.24344639   
    the 26700th batch, loss is: 0.18971057   
    the 26750th batch, loss is: 0.18854211   
    the 26800th batch, loss is: 0.16707158   
    the 26850th batch, loss is: 0.19594014   
    the 26900th batch, loss is: 0.2124079    
    the 26950th batch, loss is: 0.21287757   
('Epoch: ', 9, 'lr is: [0.001]')
    the 27000th batch, loss is: 0.16846792   
    the 27050th batch, loss is: 0.17372121   
    the 27100th batch, loss is: 0.1604193    
    the 27150th batch, loss is: 0.1370337    
    the 27200th batch, loss is: 0.17027552   
    the 27250th batch, loss is: 0.22483851   
    the 27300th batch, loss is: 0.19980599   
    the 27350th batch, loss is: 0.17064646   
    the 27400th batch, loss is: 0.1561828    
    the 27450th batch, loss is: 0.1503751    
    the 27500th batch, loss is: 0.17832148   
    the 27550th batch, loss is: 0.15277256   
    the 27600th batch, loss is: 0.16790274   
    the 27650th batch, loss is: 0.1824141    
    the 27700th batch, loss is: 0.1707138    
    the 27750th batch, loss is: 0.17481105   
    the 27800th batch, loss is: 0.18865198   
    the 27850th batch, loss is: 0.18243583   
    the 27900th batch, loss is: 0.21367759   
    the 27950th batch, loss is: 0.18565322   
    the 28000th batch, loss is: 0.15423638   
    the 28050th batch, loss is: 0.18389982   
    the 28100th batch, loss is: 0.15537091   
    the 28150th batch, loss is: 0.18161787   
    the 28200th batch, loss is: 0.19368893   
    the 28250th batch, loss is: 0.1687936    
    the 28300th batch, loss is: 0.16406365   
    the 28350th batch, loss is: 0.14771356   
    the 28400th batch, loss is: 0.17082152   
    the 28450th batch, loss is: 0.21416405   
    the 28500th batch, loss is: 0.19708337   
    the 28550th batch, loss is: 0.16601712   
    the 28600th batch, loss is: 0.19258782   
    the 28650th batch, loss is: 0.18604304   
    the 28700th batch, loss is: 0.17776407   
    the 28750th batch, loss is: 0.17831586   
    the 28800th batch, loss is: 0.20924866   
    the 28850th batch, loss is: 0.19184072   
    the 28900th batch, loss is: 0.18353258   
    the 28950th batch, loss is: 0.17621955   
    the 29000th batch, loss is: 0.18020497   
    the 29050th batch, loss is: 0.18212254   
    the 29100th batch, loss is: 0.19727202   
    the 29150th batch, loss is: 0.17019413   
    the 29200th batch, loss is: 0.18923396   
    the 29250th batch, loss is: 0.18710802   
    the 29300th batch, loss is: 0.19938351   
    the 29350th batch, loss is: 0.20208624   
    the 29400th batch, loss is: 0.20201938   
    the 29450th batch, loss is: 0.17294747   
    the 29500th batch, loss is: 0.17224982   
    the 29550th batch, loss is: 0.1573946    
    the 29600th batch, loss is: 0.17587267   
    the 29650th batch, loss is: 0.1883889    
    the 29700th batch, loss is: 0.18079671   
    the 29750th batch, loss is: 0.15245077   
    the 29800th batch, loss is: 0.18205836   
    the 29850th batch, loss is: 0.27241603   
    the 29900th batch, loss is: 0.17289177   
('Epoch: ', 10, 'lr is: [0.001]')
    the 29950th batch, loss is: 0.15580137   
    the 30000th batch, loss is: 0.15464272   
    the 30050th batch, loss is: 0.18306914   
    the 30100th batch, loss is: 0.14247432   
    the 30150th batch, loss is: 0.1541816    
    the 30200th batch, loss is: 0.13494687   
    the 30250th batch, loss is: 0.16663022   
    the 30300th batch, loss is: 0.15483272   
    the 30350th batch, loss is: 0.16224131   
    the 30400th batch, loss is: 0.16648316   
    the 30450th batch, loss is: 0.14694114   
    the 30500th batch, loss is: 0.15787111   
    the 30550th batch, loss is: 0.15715054   
    the 30600th batch, loss is: 0.16636057   
    the 30650th batch, loss is: 0.14494462   
    the 30700th batch, loss is: 0.18613967   
    the 30750th batch, loss is: 0.1448058    
    the 30800th batch, loss is: 0.16816311   
    the 30850th batch, loss is: 0.14574027   
    the 30900th batch, loss is: 0.15615979   
    the 30950th batch, loss is: 0.18946889   
    the 31000th batch, loss is: 0.15750688   
    the 31050th batch, loss is: 0.16981196   
    the 31100th batch, loss is: 0.14345595   
    the 31150th batch, loss is: 0.15820587   
    the 31200th batch, loss is: 0.14018238   
    the 31250th batch, loss is: 0.1727291    
    the 31300th batch, loss is: 0.17467256   
    the 31350th batch, loss is: 0.15444882   
    the 31400th batch, loss is: 0.17422616   
    the 31450th batch, loss is: 0.13728769   
    the 31500th batch, loss is: 0.15314247   
    the 31550th batch, loss is: 0.1597929    
    the 31600th batch, loss is: 0.17521329   
    the 31650th batch, loss is: 0.18585558   
    the 31700th batch, loss is: 0.16041803   
    the 31750th batch, loss is: 0.15694572   
    the 31800th batch, loss is: 0.17826995   
    the 31850th batch, loss is: 0.15949614   
    the 31900th batch, loss is: 0.154396     
    the 31950th batch, loss is: 0.14856392   
    the 32000th batch, loss is: 0.18309671   
    the 32050th batch, loss is: 0.15555666   
    the 32100th batch, loss is: 0.15195391   
    the 32150th batch, loss is: 0.19756769   
    the 32200th batch, loss is: 0.17548111   
    the 32250th batch, loss is: 0.16568722   
    the 32300th batch, loss is: 0.1558959    
    the 32350th batch, loss is: 0.1898106    
    the 32400th batch, loss is: 0.16136384   
    the 32450th batch, loss is: 0.15292458   
    the 32500th batch, loss is: 0.17372176   
    the 32550th batch, loss is: 0.17343166   
    the 32600th batch, loss is: 0.15716378   
    the 32650th batch, loss is: 0.15885676   
    the 32700th batch, loss is: 0.18630484   
    the 32750th batch, loss is: 0.19991441   
    the 32800th batch, loss is: 0.20012155   
    the 32850th batch, loss is: 0.16244692   
    the 32900th batch, loss is: 0.17118712   
('Epoch: ', 11, 'lr is: [0.001]')
    the 32950th batch, loss is: 0.16664582   
    the 33000th batch, loss is: 0.12870079   
    the 33050th batch, loss is: 0.13886815   
    the 33100th batch, loss is: 0.14466099   
    the 33150th batch, loss is: 0.16038617   
    the 33200th batch, loss is: 0.14259002   
    the 33250th batch, loss is: 0.15460747   
    the 33300th batch, loss is: 0.12570491   
    the 33350th batch, loss is: 0.14870588   
    the 33400th batch, loss is: 0.15187296   
    the 33450th batch, loss is: 0.15126187   
    the 33500th batch, loss is: 0.15066124   
    the 33550th batch, loss is: 0.13310272   
    the 33600th batch, loss is: 0.13550128   
    the 33650th batch, loss is: 0.13663241   
    the 33700th batch, loss is: 0.12783644   
    the 33750th batch, loss is: 0.1685304    
    the 33800th batch, loss is: 0.16028039   
    the 33850th batch, loss is: 0.14685386   
    the 33900th batch, loss is: 0.14802976   
    the 33950th batch, loss is: 0.17271626   
    the 34000th batch, loss is: 0.1417795    
    the 34050th batch, loss is: 0.15305616   
    the 34100th batch, loss is: 0.14922351   
    the 34150th batch, loss is: 0.18748792   
    the 34200th batch, loss is: 0.15118712   
    the 34250th batch, loss is: 0.1419854    
    the 34300th batch, loss is: 0.1530979    
    the 34350th batch, loss is: 0.131634     
    the 34400th batch, loss is: 0.16743352   
    the 34450th batch, loss is: 0.13490608   
    the 34500th batch, loss is: 0.13639642   
    the 34550th batch, loss is: 0.13965033   
    the 34600th batch, loss is: 0.14488375   
    the 34650th batch, loss is: 0.15101655   
    the 34700th batch, loss is: 0.18303145   
    the 34750th batch, loss is: 0.1304159    
    the 34800th batch, loss is: 0.14898482   
    the 34850th batch, loss is: 0.17518169   
    the 34900th batch, loss is: 0.16160001   
    the 34950th batch, loss is: 0.18614866   
    the 35000th batch, loss is: 0.14676128   
    the 35050th batch, loss is: 0.15390739   
    the 35100th batch, loss is: 0.14237687   
    the 35150th batch, loss is: 0.15612644   
    the 35200th batch, loss is: 0.16602963   
    the 35250th batch, loss is: 0.15209436   
    the 35300th batch, loss is: 0.15075998   
    the 35350th batch, loss is: 0.1485329    
    the 35400th batch, loss is: 0.15940082   
    the 35450th batch, loss is: 0.15613423   
    the 35500th batch, loss is: 0.15997694   
    the 35550th batch, loss is: 0.11791995   
    the 35600th batch, loss is: 0.1533819    
    the 35650th batch, loss is: 0.15919614   
    the 35700th batch, loss is: 0.15770985   
    the 35750th batch, loss is: 0.13394234   
    the 35800th batch, loss is: 0.14954339   
    the 35850th batch, loss is: 0.15543079   
    the 35900th batch, loss is: 0.16445725   
('Epoch: ', 12, 'lr is: [0.001]')
    the 35950th batch, loss is: 0.14615251   
    the 36000th batch, loss is: 0.13698024   
    the 36050th batch, loss is: 0.14313295   
    the 36100th batch, loss is: 0.12586614   
    the 36150th batch, loss is: 0.12532163   
    the 36200th batch, loss is: 0.14384477   
    the 36250th batch, loss is: 0.11426324   
    the 36300th batch, loss is: 0.12243178   
    the 36350th batch, loss is: 0.13752404   
    the 36400th batch, loss is: 0.11702511   
    the 36450th batch, loss is: 0.12528965   
    the 36500th batch, loss is: 0.12878515   
    the 36550th batch, loss is: 0.12354245   
    the 36600th batch, loss is: 0.13773547   
    the 36650th batch, loss is: 0.13169692   
    the 36700th batch, loss is: 0.12113544   
    the 36750th batch, loss is: 0.12992264   
    the 36800th batch, loss is: 0.13921326   
    the 36850th batch, loss is: 0.12925917   
    the 36900th batch, loss is: 0.14443153   
    the 36950th batch, loss is: 0.15078376   
    the 37000th batch, loss is: 0.1440002    
    the 37050th batch, loss is: 0.12945895   
    the 37100th batch, loss is: 0.15396243   
    the 37150th batch, loss is: 0.12892781   
    the 37200th batch, loss is: 0.14089453   
    the 37250th batch, loss is: 0.15824535   
    the 37300th batch, loss is: 0.14683081   
    the 37350th batch, loss is: 0.14923273   
    the 37400th batch, loss is: 0.13726906   
    the 37450th batch, loss is: 0.1550114    
    the 37500th batch, loss is: 0.13361382   
    the 37550th batch, loss is: 0.12985371   
    the 37600th batch, loss is: 0.13965748   
    the 37650th batch, loss is: 0.14333153   
    the 37700th batch, loss is: 0.13399835   
    the 37750th batch, loss is: 0.1284813    
    the 37800th batch, loss is: 0.13336125   
    the 37850th batch, loss is: 0.15033267   
    the 37900th batch, loss is: 0.13051392   
    the 37950th batch, loss is: 0.14746839   
    the 38000th batch, loss is: 0.13807485   
    the 38050th batch, loss is: 0.14321439   
    the 38100th batch, loss is: 0.16445786   
    the 38150th batch, loss is: 0.15811117   
    the 38200th batch, loss is: 0.13933879   
    the 38250th batch, loss is: 0.15516044   
    the 38300th batch, loss is: 0.14119697   
    the 38350th batch, loss is: 0.17509952   
    the 38400th batch, loss is: 0.16050671   
    the 38450th batch, loss is: 0.13981812   
    the 38500th batch, loss is: 0.14608458   
    the 38550th batch, loss is: 0.14015725   
    the 38600th batch, loss is: 0.16813162   
    the 38650th batch, loss is: 0.13267501   
    the 38700th batch, loss is: 0.13186391   
    the 38750th batch, loss is: 0.1410524    
    the 38800th batch, loss is: 0.12162826   
    the 38850th batch, loss is: 0.16084146   
    the 38900th batch, loss is: 0.11377613   
('Epoch: ', 13, 'lr is: [0.001]')
    the 38950th batch, loss is: 0.13708524   
    the 39000th batch, loss is: 0.11158732   
    the 39050th batch, loss is: 0.1029456    
    the 39100th batch, loss is: 0.12454101   
    the 39150th batch, loss is: 0.16700864   
    the 39200th batch, loss is: 0.12517159   
    the 39250th batch, loss is: 0.12221222   
    the 39300th batch, loss is: 0.12282614   
    the 39350th batch, loss is: 0.11059196   
    the 39400th batch, loss is: 0.11080773   
    the 39450th batch, loss is: 0.10907472   
    the 39500th batch, loss is: 0.13591345   
    the 39550th batch, loss is: 0.1270579    
    the 39600th batch, loss is: 0.90552849   
    the 39650th batch, loss is: 0.12626684   
    the 39700th batch, loss is: 0.11785393   
    the 39750th batch, loss is: 0.13101065   
    the 39800th batch, loss is: 0.14391436   
    the 39850th batch, loss is: 0.13430057   
    the 39900th batch, loss is: 0.10959806   
    the 39950th batch, loss is: 0.12950802   
    the 40000th batch, loss is: 0.14974093   
    the 40050th batch, loss is: 0.1224179    
    the 40100th batch, loss is: 0.1335755    
    the 40150th batch, loss is: 0.15471578   
    the 40200th batch, loss is: 0.15172574   
    the 40250th batch, loss is: 0.1160173    
    the 40300th batch, loss is: 0.12217359   
    the 40350th batch, loss is: 0.11848328   
    the 40400th batch, loss is: 0.14072622   
    the 40450th batch, loss is: 0.13284166   
    the 40500th batch, loss is: 0.11406268   
    the 40550th batch, loss is: 0.11970039   
    the 40600th batch, loss is: 0.12827127   
    the 40650th batch, loss is: 0.14085643   
    the 40700th batch, loss is: 0.17302464   
    the 40750th batch, loss is: 0.10708597   
    the 40800th batch, loss is: 0.11810993   
    the 40850th batch, loss is: 0.1452949    
    the 40900th batch, loss is: 0.13699321   
    the 40950th batch, loss is: 0.10222998   
    the 41000th batch, loss is: 0.12027064   
    the 41050th batch, loss is: 0.14038281   
    the 41100th batch, loss is: 0.15460071   
    the 41150th batch, loss is: 0.12945296   
    the 41200th batch, loss is: 0.12941994   
    the 41250th batch, loss is: 0.17162517   
    the 41300th batch, loss is: 0.13388069   
    the 41350th batch, loss is: 0.18099973   
    the 41400th batch, loss is: 0.10926643   
    the 41450th batch, loss is: 0.13862585   
    the 41500th batch, loss is: 0.14621414   
    the 41550th batch, loss is: 0.13923363   
    the 41600th batch, loss is: 0.14776184   
    the 41650th batch, loss is: 0.17567542   
    the 41700th batch, loss is: 0.13457431   
    the 41750th batch, loss is: 0.1169035    
    the 41800th batch, loss is: 0.11392077   
    the 41850th batch, loss is: 0.16105129   
    the 41900th batch, loss is: 0.15271586   
('Epoch: ', 14, 'lr is: [0.001]')
    the 41950th batch, loss is: 0.10978588   
    the 42000th batch, loss is: 0.12195352   
    the 42050th batch, loss is: 0.12793954   
    the 42100th batch, loss is: 0.13057375   
    the 42150th batch, loss is: 0.11469237   
    the 42200th batch, loss is: 0.12109734   
    the 42250th batch, loss is: 0.11879128   
    the 42300th batch, loss is: 0.09697668   
    the 42350th batch, loss is: 0.12481959   
    the 42400th batch, loss is: 0.11197721   
    the 42450th batch, loss is: 0.11180157   
    the 42500th batch, loss is: 0.10794694   
    the 42550th batch, loss is: 0.12613478   
    the 42600th batch, loss is: 0.12733564   
    the 42650th batch, loss is: 0.13141216   
    the 42700th batch, loss is: 0.1267354    
    the 42750th batch, loss is: 0.14193349   
    the 42800th batch, loss is: 0.11549983   
    the 42850th batch, loss is: 0.11649642   
    the 42900th batch, loss is: 0.11683833   
    the 42950th batch, loss is: 0.10365633   
    the 43000th batch, loss is: 0.1469744    
    the 43050th batch, loss is: 0.1274153    
    the 43100th batch, loss is: 0.10237478   
    the 43150th batch, loss is: 0.11864539   
    the 43200th batch, loss is: 0.11743253   
    the 43250th batch, loss is: 0.11804734   
    the 43300th batch, loss is: 0.12186373   
    the 43350th batch, loss is: 0.11829056   
    the 43400th batch, loss is: 0.13755909   
    the 43450th batch, loss is: 0.11457945   
    the 43500th batch, loss is: 0.12455031   
    the 43550th batch, loss is: 0.13034208   
    the 43600th batch, loss is: 0.11443079   
    the 43650th batch, loss is: 0.11041354   
    the 43700th batch, loss is: 0.12543593   
    the 43750th batch, loss is: 0.14058249   
    the 43800th batch, loss is: 0.10238472   
    the 43850th batch, loss is: 0.1167951    
    the 43900th batch, loss is: 0.11637561   
    the 43950th batch, loss is: 0.11704818   
    the 44000th batch, loss is: 0.12952311   
    the 44050th batch, loss is: 0.14834565   
    the 44100th batch, loss is: 0.13544343   
    the 44150th batch, loss is: 0.10881615   
    the 44200th batch, loss is: 0.15372314   
    the 44250th batch, loss is: 0.15227813   
    the 44300th batch, loss is: 0.12945859   
    the 44350th batch, loss is: 0.14279976   
    the 44400th batch, loss is: 0.12584752   
    the 44450th batch, loss is: 0.12265502   
    the 44500th batch, loss is: 0.12491541   
    the 44550th batch, loss is: 0.14030896   
    the 44600th batch, loss is: 0.12741718   
    the 44650th batch, loss is: 0.14663966   
    the 44700th batch, loss is: 0.11569715   
    the 44750th batch, loss is: 0.13634719   
    the 44800th batch, loss is: 0.1195338    
    the 44850th batch, loss is: 0.14119123   
    the 44900th batch, loss is: 0.12493684   
('Epoch: ', 15, 'lr is: [0.001]')
    the 44950th batch, loss is: 0.12722011   
    the 45000th batch, loss is: 0.12253546   
    the 45050th batch, loss is: 0.11101454   
    the 45100th batch, loss is: 0.11763506   
    the 45150th batch, loss is: 0.13413157   
    the 45200th batch, loss is: 0.1199012    
    the 45250th batch, loss is: 0.11444148   
    the 45300th batch, loss is: 0.10369838   
    the 45350th batch, loss is: 0.11470187   
    the 45400th batch, loss is: 0.13272266   
    the 45450th batch, loss is: 0.13084143   
    the 45500th batch, loss is: 0.13580513   
    the 45550th batch, loss is: 0.11073556   
    the 45600th batch, loss is: 0.11235758   
    the 45650th batch, loss is: 0.10589085   
    the 45700th batch, loss is: 0.11826928   
    the 45750th batch, loss is: 0.1138992    
    the 45800th batch, loss is: 0.11725248   
    the 45850th batch, loss is: 0.12523526   
    the 45900th batch, loss is: 0.10115345   
    the 45950th batch, loss is: 0.11313371   
    the 46000th batch, loss is: 0.12655698   
    the 46050th batch, loss is: 0.11092515   
    the 46100th batch, loss is: 0.11587783   
    the 46150th batch, loss is: 0.11990336   
    the 46200th batch, loss is: 0.12144241   
    the 46250th batch, loss is: 0.130284     
    the 46300th batch, loss is: 0.1222329    
    the 46350th batch, loss is: 0.09638236   
    the 46400th batch, loss is: 0.12999305   
    the 46450th batch, loss is: 0.10676801   
    the 46500th batch, loss is: 0.11071975   
    the 46550th batch, loss is: 0.12338148   
    the 46600th batch, loss is: 0.13537362   
    the 46650th batch, loss is: 0.14153051   
    the 46700th batch, loss is: 0.11548982   
    the 46750th batch, loss is: 0.12852465   
    the 46800th batch, loss is: 0.10347144   
    the 46850th batch, loss is: 0.10655262   
    the 46900th batch, loss is: 0.11043125   
    the 46950th batch, loss is: 0.13428366   
    the 47000th batch, loss is: 0.10965058   
    the 47050th batch, loss is: 0.14388679   
    the 47100th batch, loss is: 0.13782962   
    the 47150th batch, loss is: 0.12392396   
    the 47200th batch, loss is: 0.10083228   
    the 47250th batch, loss is: 0.11055588   
    the 47300th batch, loss is: 0.10977969   
    the 47350th batch, loss is: 0.13578606   
    the 47400th batch, loss is: 0.12870455   
    the 47450th batch, loss is: 0.14090192   
    the 47500th batch, loss is: 0.10395321   
    the 47550th batch, loss is: 0.11430976   
    the 47600th batch, loss is: 0.12915574   
    the 47650th batch, loss is: 0.12234969   
    the 47700th batch, loss is: 0.12340148   
    the 47750th batch, loss is: 0.11669114   
    the 47800th batch, loss is: 0.13531378   
    the 47850th batch, loss is: 0.09928358   
    the 47900th batch, loss is: 0.15376434   
('Epoch: ', 16, 'lr is: [0.001]')
    the 47950th batch, loss is: 0.09201882   
    the 48000th batch, loss is: 0.11919105   
    the 48050th batch, loss is: 0.11348379   
    the 48100th batch, loss is: 0.11476155   
    the 48150th batch, loss is: 0.12586717   
    the 48200th batch, loss is: 0.10124002   
    the 48250th batch, loss is: 0.10201889   
    the 48300th batch, loss is: 0.14289118   
    the 48350th batch, loss is: 0.10757314   
    the 48400th batch, loss is: 0.11289083   
    the 48450th batch, loss is: 0.09741998   
    the 48500th batch, loss is: 0.12303694   
    the 48550th batch, loss is: 0.1002593    
    the 48600th batch, loss is: 0.11522608   
    the 48650th batch, loss is: 0.09959953   
    the 48700th batch, loss is: 0.10936666   
    the 48750th batch, loss is: 0.10318088   
    the 48800th batch, loss is: 0.10795565   
    the 48850th batch, loss is: 0.11977583   
    the 48900th batch, loss is: 0.11259631   
    the 48950th batch, loss is: 0.10377633   
    the 49000th batch, loss is: 0.12261966   
    the 49050th batch, loss is: 0.11245178   
    the 49100th batch, loss is: 0.13579929   
    the 49150th batch, loss is: 0.10610617   
    the 49200th batch, loss is: 0.0857618    
    the 49250th batch, loss is: 0.11305773   
    the 49300th batch, loss is: 0.11475238   
    the 49350th batch, loss is: 0.10855242   
    the 49400th batch, loss is: 0.1085929    
    the 49450th batch, loss is: 0.11494813   
    the 49500th batch, loss is: 0.11245477   
    the 49550th batch, loss is: 0.1299727    
    the 49600th batch, loss is: 0.11610373   
    the 49650th batch, loss is: 0.12037106   
    the 49700th batch, loss is: 0.11762584   
    the 49750th batch, loss is: 0.10925883   
    the 49800th batch, loss is: 0.14203657   
    the 49850th batch, loss is: 0.13205586   
    the 49900th batch, loss is: 0.13342465   
    the 49950th batch, loss is: 0.1162876    
    the 50000th batch, loss is: 0.10319132   
    the 50050th batch, loss is: 0.10997749   
    the 50100th batch, loss is: 0.12262765   
    the 50150th batch, loss is: 0.12224414   
    the 50200th batch, loss is: 0.12717505   
    the 50250th batch, loss is: 0.12305265   
    the 50300th batch, loss is: 0.09904943   
    the 50350th batch, loss is: 0.14103782   
    the 50400th batch, loss is: 0.11262531   
    the 50450th batch, loss is: 0.10629261   
    the 50500th batch, loss is: 0.13746536   
    the 50550th batch, loss is: 0.11836478   
    the 50600th batch, loss is: 0.11659138   
    the 50650th batch, loss is: 0.10529745   
    the 50700th batch, loss is: 0.10878272   
    the 50750th batch, loss is: 0.11407754   
    the 50800th batch, loss is: 0.13917622   
    the 50850th batch, loss is: 0.10257629   
    the 50900th batch, loss is: 0.12262099   
('Epoch: ', 17, 'lr is: [0.001]')
    the 50950th batch, loss is: 0.11239579   
    the 51000th batch, loss is: 0.10516963   
    the 51050th batch, loss is: 0.09509865   
    the 51100th batch, loss is: 0.13238712   
    the 51150th batch, loss is: 0.09807287   
    the 51200th batch, loss is: 0.10467026   
    the 51250th batch, loss is: 0.09751301   
    the 51300th batch, loss is: 0.12286576   
    the 51350th batch, loss is: 0.08634452   
    the 51400th batch, loss is: 0.09695945   
    the 51450th batch, loss is: 0.11346626   
    the 51500th batch, loss is: 0.08798365   
    the 51550th batch, loss is: 0.10165752   
    the 51600th batch, loss is: 0.11206315   
    the 51650th batch, loss is: 0.09466708   
    the 51700th batch, loss is: 0.1119399    
    the 51750th batch, loss is: 0.10223385   
    the 51800th batch, loss is: 0.10070226   
    the 51850th batch, loss is: 0.11292798   
    the 51900th batch, loss is: 0.09625526   
    the 51950th batch, loss is: 0.10335864   
    the 52000th batch, loss is: 0.09819107   
    the 52050th batch, loss is: 0.13022476   
    the 52100th batch, loss is: 0.1003206    
    the 52150th batch, loss is: 0.09847494   
    the 52200th batch, loss is: 0.10947734   
    the 52250th batch, loss is: 0.12365717   
    the 52300th batch, loss is: 0.11800978   
    the 52350th batch, loss is: 0.10471793   
    the 52400th batch, loss is: 0.11584505   
    the 52450th batch, loss is: 0.11555971   
    the 52500th batch, loss is: 0.09363532   
    the 52550th batch, loss is: 0.11521366   
    the 52600th batch, loss is: 0.11577717   
    the 52650th batch, loss is: 0.12241188   
    the 52700th batch, loss is: 0.12245588   
    the 52750th batch, loss is: 0.12205233   
    the 52800th batch, loss is: 0.11482526   
    the 52850th batch, loss is: 0.12768568   
    the 52900th batch, loss is: 0.09390783   
    the 52950th batch, loss is: 0.0933313    
    the 53000th batch, loss is: 0.13213494   
    the 53050th batch, loss is: 0.10900641   
    the 53100th batch, loss is: 0.1350255    
    the 53150th batch, loss is: 0.12384456   
    the 53200th batch, loss is: 0.1257311    
    the 53250th batch, loss is: 0.12489641   
    the 53300th batch, loss is: 0.09907541   
    the 53350th batch, loss is: 0.11283488   
    the 53400th batch, loss is: 0.11064335   
    the 53450th batch, loss is: 0.10833868   
    the 53500th batch, loss is: 0.10843088   
    the 53550th batch, loss is: 0.10882087   
    the 53600th batch, loss is: 0.11583677   
    the 53650th batch, loss is: 0.13391897   
    the 53700th batch, loss is: 0.12788755   
    the 53750th batch, loss is: 0.11511543   
    the 53800th batch, loss is: 0.1033076    
    the 53850th batch, loss is: 0.11022613   
    the 53900th batch, loss is: 0.10818662   
('Epoch: ', 18, 'lr is: [0.001]')
    the 53950th batch, loss is: 0.10199455   
    the 54000th batch, loss is: 0.1036097    
    the 54050th batch, loss is: 0.11011588   
    the 54100th batch, loss is: 0.09316391   
    the 54150th batch, loss is: 0.09122007   
    the 54200th batch, loss is: 0.10718122   
    the 54250th batch, loss is: 0.1002083    
    the 54300th batch, loss is: 0.09969307   
    the 54350th batch, loss is: 0.09942897   
    the 54400th batch, loss is: 0.10805987   
    the 54450th batch, loss is: 0.08969468   
    the 54500th batch, loss is: 0.11620854   
    the 54550th batch, loss is: 0.08590712   
    the 54600th batch, loss is: 0.08573813   
    the 54650th batch, loss is: 0.11529025   
    the 54700th batch, loss is: 0.092261     
    the 54750th batch, loss is: 0.0951123    
    the 54800th batch, loss is: 0.09239766   
    the 54850th batch, loss is: 0.08716608   
    the 54900th batch, loss is: 0.09372815   
    the 54950th batch, loss is: 0.12033827   
    the 55000th batch, loss is: 0.11771419   
    the 55050th batch, loss is: 0.09141878   
    the 55100th batch, loss is: 0.09637779   
    the 55150th batch, loss is: 0.11169842   
    the 55200th batch, loss is: 0.09714579   
    the 55250th batch, loss is: 0.10160657   
    the 55300th batch, loss is: 0.10725544   
    the 55350th batch, loss is: 0.24957091   
    the 55400th batch, loss is: 0.10668879   
    the 55450th batch, loss is: 0.08777043   
    the 55500th batch, loss is: 0.10343576   
    the 55550th batch, loss is: 0.1043831    
    the 55600th batch, loss is: 0.10524672   
    the 55650th batch, loss is: 0.08964704   
    the 55700th batch, loss is: 0.12166783   
    the 55750th batch, loss is: 0.10836118   
    the 55800th batch, loss is: 0.10579254   
    the 55850th batch, loss is: 0.12700479   
    the 55900th batch, loss is: 0.10963967   
    the 55950th batch, loss is: 0.09179956   
    the 56000th batch, loss is: 0.08816554   
    the 56050th batch, loss is: 0.09606023   
    the 56100th batch, loss is: 0.09064174   
    the 56150th batch, loss is: 0.09258614   
    the 56200th batch, loss is: 0.12087063   
    the 56250th batch, loss is: 0.08887083   
    the 56300th batch, loss is: 0.11561806   
    the 56350th batch, loss is: 0.08830015   
    the 56400th batch, loss is: 0.10976012   
    the 56450th batch, loss is: 0.1214686    
    the 56500th batch, loss is: 0.09979858   
    the 56550th batch, loss is: 0.13047135   
    the 56600th batch, loss is: 0.10988174   
    the 56650th batch, loss is: 0.12660749   
    the 56700th batch, loss is: 0.08211762   
    the 56750th batch, loss is: 0.12785004   
    the 56800th batch, loss is: 0.10409327   
    the 56850th batch, loss is: 0.09906735   
    the 56900th batch, loss is: 0.13720074   
('Epoch: ', 19, 'lr is: [0.001]')
    the 56950th batch, loss is: 0.08865833   
    the 57000th batch, loss is: 0.14373755   
    the 57050th batch, loss is: 0.08814638   
    the 57100th batch, loss is: 0.0940014    
    the 57150th batch, loss is: 0.0919363    
    the 57200th batch, loss is: 0.0973618    
    the 57250th batch, loss is: 0.07919627   
    the 57300th batch, loss is: 0.10627998   
    the 57350th batch, loss is: 0.09493004   
    the 57400th batch, loss is: 0.08156936   
    the 57450th batch, loss is: 0.0988175    
    the 57500th batch, loss is: 0.12553857   
    the 57550th batch, loss is: 0.10507168   
    the 57600th batch, loss is: 0.08688436   
    the 57650th batch, loss is: 0.08711861   
    the 57700th batch, loss is: 0.09654778   
    the 57750th batch, loss is: 0.10854645   
    the 57800th batch, loss is: 0.10473517   
    the 57850th batch, loss is: 0.09489617   
    the 57900th batch, loss is: 0.09542069   
    the 57950th batch, loss is: 0.08198753   
    the 58000th batch, loss is: 0.09773774   
    the 58050th batch, loss is: 0.08966725   
    the 58100th batch, loss is: 0.09986944   
    the 58150th batch, loss is: 0.11748954   
    the 58200th batch, loss is: 0.09256761   
    the 58250th batch, loss is: 0.08885629   
    the 58300th batch, loss is: 0.09139112   
    the 58350th batch, loss is: 0.09661715   
    the 58400th batch, loss is: 0.09169914   
    the 58450th batch, loss is: 0.09229512   
    the 58500th batch, loss is: 0.10131074   
    the 58550th batch, loss is: 0.09941433   
    the 58600th batch, loss is: 0.1171258    
    the 58650th batch, loss is: 0.10507601   
    the 58700th batch, loss is: 0.1002019    
    the 58750th batch, loss is: 0.09710563   
    the 58800th batch, loss is: 0.10168914   
    the 58850th batch, loss is: 0.09826727   
    the 58900th batch, loss is: 0.10152531   
    the 58950th batch, loss is: 0.10238007   
    the 59000th batch, loss is: 0.11948122   
    the 59050th batch, loss is: 0.09609493   
    the 59100th batch, loss is: 0.10254953   
    the 59150th batch, loss is: 0.09892198   
    the 59200th batch, loss is: 0.10647481   
    the 59250th batch, loss is: 0.09239659   
    the 59300th batch, loss is: 0.10650652   
    the 59350th batch, loss is: 0.08236428   
    the 59400th batch, loss is: 0.11011931   
    the 59450th batch, loss is: 0.09734321   
    the 59500th batch, loss is: 0.11787755   
    the 59550th batch, loss is: 0.10753777   
    the 59600th batch, loss is: 0.08065784   
    the 59650th batch, loss is: 0.11414236   
    the 59700th batch, loss is: 0.10668658   
    the 59750th batch, loss is: 0.09757638   
    the 59800th batch, loss is: 0.11856049   
    the 59850th batch, loss is: 0.10296162   
('Epoch: ', 20, 'lr is: [0.001]')
    the 59900th batch, loss is: 0.08668293   
    the 59950th batch, loss is: 0.09543352   
    the 60000th batch, loss is: 0.09218633   
    the 60050th batch, loss is: 0.08571731   
    the 60100th batch, loss is: 0.09503832   
    the 60150th batch, loss is: 0.09065425   
    the 60200th batch, loss is: 0.10032648   
    the 60250th batch, loss is: 0.08226866   
    the 60300th batch, loss is: 0.09731648   
    the 60350th batch, loss is: 0.09506921   
    the 60400th batch, loss is: 0.08944187   
    the 60450th batch, loss is: 0.09663418   
    the 60500th batch, loss is: 0.0857845    
    the 60550th batch, loss is: 0.10822912   
    the 60600th batch, loss is: 0.08447748   
    the 60650th batch, loss is: 0.11475859   
    the 60700th batch, loss is: 0.11234024   
    the 60750th batch, loss is: 0.09201391   
    the 60800th batch, loss is: 0.09169675   
    the 60850th batch, loss is: 0.108784     
    the 60900th batch, loss is: 0.09286247   
    the 60950th batch, loss is: 0.09486013   
    the 61000th batch, loss is: 0.09262576   
    the 61050th batch, loss is: 0.10011821   
    the 61100th batch, loss is: 0.11282848   
    the 61150th batch, loss is: 0.11624135   
    the 61200th batch, loss is: 0.08719551   
    the 61250th batch, loss is: 0.08460455   
    the 61300th batch, loss is: 0.09063531   
    the 61350th batch, loss is: 0.09569526   
    the 61400th batch, loss is: 0.12801565   
    the 61450th batch, loss is: 0.09169562   
    the 61500th batch, loss is: 0.09097973   
    the 61550th batch, loss is: 0.07950917   
    the 61600th batch, loss is: 0.08081125   
    the 61650th batch, loss is: 0.09095977   
    the 61700th batch, loss is: 0.08803235   
    the 61750th batch, loss is: 0.09539808   
    the 61800th batch, loss is: 0.10713562   
    the 61850th batch, loss is: 0.11118622   
    the 61900th batch, loss is: 0.0955956    
    the 61950th batch, loss is: 0.10202684   
    the 62000th batch, loss is: 0.10468477   
    the 62050th batch, loss is: 0.11326862   
    the 62100th batch, loss is: 0.0987451    
    the 62150th batch, loss is: 0.09482223   
    the 62200th batch, loss is: 0.10534955   
    the 62250th batch, loss is: 0.09488353   
    the 62300th batch, loss is: 0.10319716   
    the 62350th batch, loss is: 0.10209113   
    the 62400th batch, loss is: 0.08811791   
    the 62450th batch, loss is: 0.10722572   
    the 62500th batch, loss is: 0.08990236   
    the 62550th batch, loss is: 0.09581574   
    the 62600th batch, loss is: 0.0831278    
    the 62650th batch, loss is: 0.09992938   
    the 62700th batch, loss is: 0.10411502   
    the 62750th batch, loss is: 0.0948483    
    the 62800th batch, loss is: 0.13188693   
    the 62850th batch, loss is: 0.0961483    
('Epoch: ', 21, 'lr is: [0.001]')
    the 62900th batch, loss is: 0.09560145   
    the 62950th batch, loss is: 0.09131853   
    the 63000th batch, loss is: 0.09380563   
    the 63050th batch, loss is: 0.09250743   
    the 63100th batch, loss is: 0.07752667   
    the 63150th batch, loss is: 0.08856467   
    the 63200th batch, loss is: 0.09472946   
    the 63250th batch, loss is: 0.08595306   
    the 63300th batch, loss is: 0.08238488   
    the 63350th batch, loss is: 0.12339914   
    the 63400th batch, loss is: 0.09870321   
    the 63450th batch, loss is: 0.07996952   
    the 63500th batch, loss is: 0.06108574   
    the 63550th batch, loss is: 0.08932429   
    the 63600th batch, loss is: 0.10674068   
    the 63650th batch, loss is: 0.09820203   
    the 63700th batch, loss is: 0.0916192    
    the 63750th batch, loss is: 0.08608948   
    the 63800th batch, loss is: 0.12436075   
    the 63850th batch, loss is: 0.07996573   
    the 63900th batch, loss is: 0.1068214    
    the 63950th batch, loss is: 0.09430031   
    the 64000th batch, loss is: 0.10277068   
    the 64050th batch, loss is: 0.09249207   
    the 64100th batch, loss is: 0.10679314   
    the 64150th batch, loss is: 0.0937984    
    the 64200th batch, loss is: 0.10186817   
    the 64250th batch, loss is: 0.09621724   
    the 64300th batch, loss is: 0.0827678    
    the 64350th batch, loss is: 0.07718715   
    the 64400th batch, loss is: 0.08603655   
    the 64450th batch, loss is: 0.08819516   
    the 64500th batch, loss is: 0.09974451   
    the 64550th batch, loss is: 0.09923812   
    the 64600th batch, loss is: 0.09110937   
    the 64650th batch, loss is: 0.0951684    
    the 64700th batch, loss is: 0.08276562   
    the 64750th batch, loss is: 0.10428032   
    the 64800th batch, loss is: 0.08798689   
    the 64850th batch, loss is: 0.09862881   
    the 64900th batch, loss is: 0.09759105   
    the 64950th batch, loss is: 0.09839725   
    the 65000th batch, loss is: 0.09029994   
    the 65050th batch, loss is: 0.11936627   
    the 65100th batch, loss is: 0.07563866   
    the 65150th batch, loss is: 0.10052215   
    the 65200th batch, loss is: 0.09334078   
    the 65250th batch, loss is: 0.11231098   
    the 65300th batch, loss is: 0.09383276   
    the 65350th batch, loss is: 0.10035551   
    the 65400th batch, loss is: 0.09064095   
    the 65450th batch, loss is: 0.10000434   
    the 65500th batch, loss is: 0.0992758    
    the 65550th batch, loss is: 0.084217     
    the 65600th batch, loss is: 0.08406843   
    the 65650th batch, loss is: 0.08959859   
    the 65700th batch, loss is: 0.10458613   
    the 65750th batch, loss is: 0.09498215   
    the 65800th batch, loss is: 0.11438157   
    the 65850th batch, loss is: 0.09316105   
('Epoch: ', 22, 'lr is: [0.001]')
    the 65900th batch, loss is: 0.08323576   
    the 65950th batch, loss is: 0.07292656   
    the 66000th batch, loss is: 0.08768754   
    the 66050th batch, loss is: 0.10203291   
    the 66100th batch, loss is: 0.08612357   
    the 66150th batch, loss is: 0.09758429   
    the 66200th batch, loss is: 0.08846604   
    the 66250th batch, loss is: 0.08936589   
    the 66300th batch, loss is: 0.08187152   
    the 66350th batch, loss is: 0.0853788    
    the 66400th batch, loss is: 0.08974027   
    the 66450th batch, loss is: 0.07363219   
    the 66500th batch, loss is: 0.09491231   
    the 66550th batch, loss is: 0.09697805   
    the 66600th batch, loss is: 0.09837788   
    the 66650th batch, loss is: 0.08164996   
    the 66700th batch, loss is: 0.08856961   
    the 66750th batch, loss is: 0.07741686   
    the 66800th batch, loss is: 0.09050246   
    the 66850th batch, loss is: 0.10235126   
    the 66900th batch, loss is: 0.08235      
    the 66950th batch, loss is: 0.08599024   
    the 67000th batch, loss is: 0.09352998   
    the 67050th batch, loss is: 0.07777817   
    the 67100th batch, loss is: 0.08723816   
    the 67150th batch, loss is: 0.08101927   
    the 67200th batch, loss is: 0.08063827   
    the 67250th batch, loss is: 0.09580476   
    the 67300th batch, loss is: 0.09731282   
    the 67350th batch, loss is: 0.08667865   
    the 67400th batch, loss is: 0.09069211   
    the 67450th batch, loss is: 0.09479347   
    the 67500th batch, loss is: 0.09156512   
    the 67550th batch, loss is: 0.10885724   
    the 67600th batch, loss is: 0.0982745    
    the 67650th batch, loss is: 0.09671605   
    the 67700th batch, loss is: 0.07463042   
    the 67750th batch, loss is: 0.08521283   
    the 67800th batch, loss is: 0.08165273   
    the 67850th batch, loss is: 0.09722006   
    the 67900th batch, loss is: 0.10158395   
    the 67950th batch, loss is: 0.08522819   
    the 68000th batch, loss is: 0.09287477   
    the 68050th batch, loss is: 0.08346611   
    the 68100th batch, loss is: 0.07691295   
    the 68150th batch, loss is: 0.08561578   
    the 68200th batch, loss is: 0.08537062   
    the 68250th batch, loss is: 0.08423923   
    the 68300th batch, loss is: 0.09519536   
    the 68350th batch, loss is: 0.08977207   
    the 68400th batch, loss is: 0.09298941   
    the 68450th batch, loss is: 0.08848826   
    the 68500th batch, loss is: 0.09530763   
    the 68550th batch, loss is: 0.09649471   
    the 68600th batch, loss is: 0.0867691    
    the 68650th batch, loss is: 0.0940259    
    the 68700th batch, loss is: 0.09291081   
    the 68750th batch, loss is: 0.093724     
    the 68800th batch, loss is: 0.09646624   
    the 68850th batch, loss is: 0.09678335   
('Epoch: ', 23, 'lr is: [0.001]')
    the 68900th batch, loss is: 0.08748675   
    the 68950th batch, loss is: 0.07962803   
    the 69000th batch, loss is: 0.07903076   
    the 69050th batch, loss is: 0.08315735   
    the 69100th batch, loss is: 0.08226006   
    the 69150th batch, loss is: 0.08906709   
    the 69200th batch, loss is: 0.07676622   
    the 69250th batch, loss is: 0.08454134   
    the 69300th batch, loss is: 0.08339243   
    the 69350th batch, loss is: 0.08674002   
    the 69400th batch, loss is: 0.08162095   
    the 69450th batch, loss is: 0.0816915    
    the 69500th batch, loss is: 0.08875566   
    the 69550th batch, loss is: 0.08192604   
    the 69600th batch, loss is: 0.09663036   
    the 69650th batch, loss is: 0.08103096   
    the 69700th batch, loss is: 0.07472301   
    the 69750th batch, loss is: 0.08102971   
    the 69800th batch, loss is: 0.08367976   
    the 69850th batch, loss is: 0.09248658   
    the 69900th batch, loss is: 0.07313149   
    the 69950th batch, loss is: 0.07921977   
    the 70000th batch, loss is: 0.09277564   
    the 70050th batch, loss is: 0.08201484   
    the 70100th batch, loss is: 0.07368051   
    the 70150th batch, loss is: 0.08796039   
    the 70200th batch, loss is: 0.08479933   
    the 70250th batch, loss is: 0.1022987    
    the 70300th batch, loss is: 0.0845241    
    the 70350th batch, loss is: 0.07500342   
    the 70400th batch, loss is: 0.07904935   
    the 70450th batch, loss is: 0.08026871   
    the 70500th batch, loss is: 0.11015394   
    the 70550th batch, loss is: 0.09480759   
    the 70600th batch, loss is: 0.11643511   
    the 70650th batch, loss is: 0.08294318   
    the 70700th batch, loss is: 0.09250269   
    the 70750th batch, loss is: 0.08586347   
    the 70800th batch, loss is: 0.09132531   
    the 70850th batch, loss is: 0.10841997   
    the 70900th batch, loss is: 0.09157316   
    the 70950th batch, loss is: 0.089997     
    the 71000th batch, loss is: 0.09166454   
    the 71050th batch, loss is: 0.09401481   
    the 71100th batch, loss is: 0.10111102   
    the 71150th batch, loss is: 0.0799122    
    the 71200th batch, loss is: 0.08087405   
    the 71250th batch, loss is: 0.09841096   
    the 71300th batch, loss is: 0.07943473   
    the 71350th batch, loss is: 0.09649219   
    the 71400th batch, loss is: 0.10335666   
    the 71450th batch, loss is: 0.09957096   
    the 71500th batch, loss is: 0.08687047   
    the 71550th batch, loss is: 0.08994792   
    the 71600th batch, loss is: 0.09731342   
    the 71650th batch, loss is: 0.09281402   
    the 71700th batch, loss is: 0.08585428   
    the 71750th batch, loss is: 0.09581887   
    the 71800th batch, loss is: 0.10027417   
    the 71850th batch, loss is: 0.08566012   
('Epoch: ', 24, 'lr is: [0.001]')
    the 71900th batch, loss is: 0.08449212   
    the 71950th batch, loss is: 0.16711743   
    the 72000th batch, loss is: 0.09388214   
    the 72050th batch, loss is: 0.09967764   
    the 72100th batch, loss is: 0.07945901   
    the 72150th batch, loss is: 0.08515306   
    the 72200th batch, loss is: 0.07883868   
    the 72250th batch, loss is: 0.09086725   
    the 72300th batch, loss is: 0.0899964    
    the 72350th batch, loss is: 0.08888084   
    the 72400th batch, loss is: 0.09725314   
    the 72450th batch, loss is: 0.08831534   
    the 72500th batch, loss is: 0.07746101   
    the 72550th batch, loss is: 0.08550586   
    the 72600th batch, loss is: 0.09613404   
    the 72650th batch, loss is: 0.0806577    
    the 72700th batch, loss is: 0.10583182   
    the 72750th batch, loss is: 0.09358101   
    the 72800th batch, loss is: 0.09238243   
    the 72850th batch, loss is: 0.09030747   
    the 72900th batch, loss is: 0.07342513   
    the 72950th batch, loss is: 0.09368885   
    the 73000th batch, loss is: 0.07045762   
    the 73050th batch, loss is: 0.07598778   
    the 73100th batch, loss is: 0.07735001   
    the 73150th batch, loss is: 0.08691087   
    the 73200th batch, loss is: 0.08925781   
    the 73250th batch, loss is: 0.08735602   
    the 73300th batch, loss is: 0.07071363   
    the 73350th batch, loss is: 0.09234632   
    the 73400th batch, loss is: 0.08076403   
    the 73450th batch, loss is: 0.08909971   
    the 73500th batch, loss is: 0.07125186   
    the 73550th batch, loss is: 0.11251158   
    the 73600th batch, loss is: 0.09514731   
    the 73650th batch, loss is: 0.10949318   
    the 73700th batch, loss is: 0.08571456   
    the 73750th batch, loss is: 0.07649609   
    the 73800th batch, loss is: 0.09164316   
    the 73850th batch, loss is: 0.08649126   
    the 73900th batch, loss is: 0.07611555   
    the 73950th batch, loss is: 0.08302356   
    the 74000th batch, loss is: 0.08669011   
    the 74050th batch, loss is: 0.06988218   
    the 74100th batch, loss is: 0.08928352   
    the 74150th batch, loss is: 0.07218096   
    the 74200th batch, loss is: 0.08032226   
    the 74250th batch, loss is: 0.09524947   
    the 74300th batch, loss is: 0.07953969   
    the 74350th batch, loss is: 0.08791235   
    the 74400th batch, loss is: 0.07439455   
    the 74450th batch, loss is: 0.1023096    
    the 74500th batch, loss is: 0.09566048   
    the 74550th batch, loss is: 0.07414829   
    the 74600th batch, loss is: 0.08406309   
    the 74650th batch, loss is: 0.08132086   
    the 74700th batch, loss is: 0.07550108   
    the 74750th batch, loss is: 0.07566891   
    the 74800th batch, loss is: 0.08194404   
    the 74850th batch, loss is: 0.10034025   
('Epoch: ', 25, 'lr is: [0.001]')
    the 74900th batch, loss is: 0.07310537   
    the 74950th batch, loss is: 0.08062636   
    the 75000th batch, loss is: 0.08206316   
    the 75050th batch, loss is: 0.07352813   
    the 75100th batch, loss is: 0.07573568   
    the 75150th batch, loss is: 0.07353142   
    the 75200th batch, loss is: 0.08238784   
    the 75250th batch, loss is: 0.07026046   
    the 75300th batch, loss is: 0.08035712   
    the 75350th batch, loss is: 0.08479327   
    the 75400th batch, loss is: 0.08571392   
    the 75450th batch, loss is: 0.08420965   
    the 75500th batch, loss is: 0.08475503   
    the 75550th batch, loss is: 0.06938685   
    the 75600th batch, loss is: 0.08140817   
    the 75650th batch, loss is: 0.0993125    
    the 75700th batch, loss is: 0.07400569   
    the 75750th batch, loss is: 0.08919857   
    the 75800th batch, loss is: 0.06979603   
    the 75850th batch, loss is: 0.07335743   
    the 75900th batch, loss is: 0.08843662   
    the 75950th batch, loss is: 0.08347046   
    the 76000th batch, loss is: 0.08941148   
    the 76050th batch, loss is: 0.07019933   
    the 76100th batch, loss is: 0.08658478   
    the 76150th batch, loss is: 0.08367363   
    the 76200th batch, loss is: 0.08409823   
    the 76250th batch, loss is: 0.06742401   
    the 76300th batch, loss is: 0.08652938   
    the 76350th batch, loss is: 0.08975773   
    the 76400th batch, loss is: 0.089295     
    the 76450th batch, loss is: 0.07568825   
    the 76500th batch, loss is: 0.08215345   
    the 76550th batch, loss is: 0.09045292   
    the 76600th batch, loss is: 0.10031071   
    the 76650th batch, loss is: 0.09143999   
    the 76700th batch, loss is: 0.07887658   
    the 76750th batch, loss is: 0.10563149   
    the 76800th batch, loss is: 0.09201957   
    the 76850th batch, loss is: 0.08227465   
    the 76900th batch, loss is: 0.08461069   
    the 76950th batch, loss is: 0.08868958   
    the 77000th batch, loss is: 0.07906151   
    the 77050th batch, loss is: 0.10076729   
    the 77100th batch, loss is: 0.08741222   
    the 77150th batch, loss is: 0.08180543   
    the 77200th batch, loss is: 0.08150275   
    the 77250th batch, loss is: 0.09770248   
    the 77300th batch, loss is: 0.07588217   
    the 77350th batch, loss is: 0.09915745   
    the 77400th batch, loss is: 0.08830083   
    the 77450th batch, loss is: 0.08547119   
    the 77500th batch, loss is: 0.08738068   
    the 77550th batch, loss is: 0.09398323   
    the 77600th batch, loss is: 0.0795471    
    the 77650th batch, loss is: 0.08878662   
    the 77700th batch, loss is: 0.09957799   
    the 77750th batch, loss is: 0.08854404   
    the 77800th batch, loss is: 0.08732416   
    the 77850th batch, loss is: 0.08236166   
('Epoch: ', 26, 'lr is: [0.001]')
    the 77900th batch, loss is: 0.08822656   
    the 77950th batch, loss is: 0.07524383   
    the 78000th batch, loss is: 0.09539064   
    the 78050th batch, loss is: 0.08851303   
    the 78100th batch, loss is: 0.07532211   
    the 78150th batch, loss is: 0.08945277   
    the 78200th batch, loss is: 0.08160365   
    the 78250th batch, loss is: 0.07132695   
    the 78300th batch, loss is: 0.08001933   
    the 78350th batch, loss is: 0.06904819   
    the 78400th batch, loss is: 0.08513198   
    the 78450th batch, loss is: 0.08342582   
    the 78500th batch, loss is: 0.08408821   
    the 78550th batch, loss is: 0.08420669   
    the 78600th batch, loss is: 0.09521431   
    the 78650th batch, loss is: 0.08900396   
    the 78700th batch, loss is: 0.08220841   
    the 78750th batch, loss is: 0.07463624   
    the 78800th batch, loss is: 0.08034179   
    the 78850th batch, loss is: 0.08427054   
    the 78900th batch, loss is: 0.08097709   
    the 78950th batch, loss is: 0.09066808   
    the 79000th batch, loss is: 0.07458407   
    the 79050th batch, loss is: 0.08877857   
    the 79100th batch, loss is: 0.08550167   
    the 79150th batch, loss is: 0.08016208   
    the 79200th batch, loss is: 0.0777174    
    the 79250th batch, loss is: 0.09051048   
    the 79300th batch, loss is: 0.08026896   
    the 79350th batch, loss is: 0.07854621   
    the 79400th batch, loss is: 0.08641661   
    the 79450th batch, loss is: 0.07942864   
    the 79500th batch, loss is: 0.07914739   
    the 79550th batch, loss is: 0.0746148    
    the 79600th batch, loss is: 0.07890529   
    the 79650th batch, loss is: 0.06882211   
    the 79700th batch, loss is: 0.08933866   
    the 79750th batch, loss is: 0.08757208   
    the 79800th batch, loss is: 0.08046409   
    the 79850th batch, loss is: 0.08301073   
    the 79900th batch, loss is: 0.08129383   
    the 79950th batch, loss is: 0.10248462   
    the 80000th batch, loss is: 0.0860046    
    the 80050th batch, loss is: 0.08340405   
    the 80100th batch, loss is: 0.07853975   
    the 80150th batch, loss is: 0.07733004   
    the 80200th batch, loss is: 0.09800909   
    the 80250th batch, loss is: 0.0880483    
    the 80300th batch, loss is: 0.08689314   
    the 80350th batch, loss is: 0.08838288   
    the 80400th batch, loss is: 0.08496545   
    the 80450th batch, loss is: 0.06362274   
    the 80500th batch, loss is: 0.09755585   
    the 80550th batch, loss is: 0.07945018   
    the 80600th batch, loss is: 0.07794456   
    the 80650th batch, loss is: 0.07491688   
    the 80700th batch, loss is: 0.08551612   
    the 80750th batch, loss is: 0.07075761   
    the 80800th batch, loss is: 0.08810712   
    the 80850th batch, loss is: 0.07608464   
('Epoch: ', 27, 'lr is: [0.001]')
    the 80900th batch, loss is: 0.09457062   
    the 80950th batch, loss is: 0.07337997   
    the 81000th batch, loss is: 0.07983647   
    the 81050th batch, loss is: 0.0842837    
    the 81100th batch, loss is: 0.07886749   
    the 81150th batch, loss is: 0.08791148   
    the 81200th batch, loss is: 0.07487331   
    the 81250th batch, loss is: 0.10458232   
    the 81300th batch, loss is: 0.12597921   
    the 81350th batch, loss is: 0.08721396   
    the 81400th batch, loss is: 0.07584706   
    the 81450th batch, loss is: 0.07259511   
    the 81500th batch, loss is: 0.08517034   
    the 81550th batch, loss is: 0.10362467   
    the 81600th batch, loss is: 0.06815093   
    the 81650th batch, loss is: 0.08240873   
    the 81700th batch, loss is: 0.07042839   
    the 81750th batch, loss is: 0.07840981   
    the 81800th batch, loss is: 0.09435898   
    the 81850th batch, loss is: 0.06753473   
    the 81900th batch, loss is: 0.07590955   
    the 81950th batch, loss is: 0.08065235   
    the 82000th batch, loss is: 0.07287851   
    the 82050th batch, loss is: 0.07115927   
    the 82100th batch, loss is: 0.08139118   
    the 82150th batch, loss is: 0.08280731   
    the 82200th batch, loss is: 0.12357219   
    the 82250th batch, loss is: 0.09525681   
    the 82300th batch, loss is: 0.06942692   
    the 82350th batch, loss is: 0.08387479   
    the 82400th batch, loss is: 0.08354385   
    the 82450th batch, loss is: 0.09142216   
    the 82500th batch, loss is: 0.09003491   
    the 82550th batch, loss is: 0.07598944   
    the 82600th batch, loss is: 0.09667111   
    the 82650th batch, loss is: 0.07448152   
    the 82700th batch, loss is: 0.0994537    
    the 82750th batch, loss is: 0.07487199   
    the 82800th batch, loss is: 0.08576701   
    the 82850th batch, loss is: 0.06536064   
    the 82900th batch, loss is: 0.09961949   
    the 82950th batch, loss is: 0.086547     
    the 83000th batch, loss is: 0.09010338   
    the 83050th batch, loss is: 0.08066088   
    the 83100th batch, loss is: 0.08398026   
    the 83150th batch, loss is: 0.09414063   
    the 83200th batch, loss is: 0.08367973   
    the 83250th batch, loss is: 0.08660903   
    the 83300th batch, loss is: 0.08887129   
    the 83350th batch, loss is: 0.07026287   
    the 83400th batch, loss is: 0.10237388   
    the 83450th batch, loss is: 0.10465954   
    the 83500th batch, loss is: 0.0725597    
    the 83550th batch, loss is: 0.08738317   
    the 83600th batch, loss is: 0.09405187   
    the 83650th batch, loss is: 0.08504471   
    the 83700th batch, loss is: 0.0913935    
    the 83750th batch, loss is: 0.08463974   
    the 83800th batch, loss is: 0.07739431   
    the 83850th batch, loss is: 0.08948574   
('Epoch: ', 28, 'lr is: [0.001]')
    the 83900th batch, loss is: 0.07438948   
    the 83950th batch, loss is: 0.07878157   
    the 84000th batch, loss is: 0.07825815   
    the 84050th batch, loss is: 0.07978483   
    the 84100th batch, loss is: 0.06849804   
    the 84150th batch, loss is: 0.0895246    
    the 84200th batch, loss is: 0.07507928   
    the 84250th batch, loss is: 0.07650559   
    the 84300th batch, loss is: 0.08438741   
    the 84350th batch, loss is: 0.08071393   
    the 84400th batch, loss is: 0.07887184   
    the 84450th batch, loss is: 0.06913953   
    the 84500th batch, loss is: 0.07245316   
    the 84550th batch, loss is: 0.06848083   
    the 84600th batch, loss is: 0.07423676   
    the 84650th batch, loss is: 0.07563597   
    the 84700th batch, loss is: 0.07955538   
    the 84750th batch, loss is: 0.08989441   
    the 84800th batch, loss is: 0.07796893   
    the 84850th batch, loss is: 0.07064241   
    the 84900th batch, loss is: 0.07428418   
    the 84950th batch, loss is: 0.08136649   
    the 85000th batch, loss is: 0.07768854   
    the 85050th batch, loss is: 0.08021829   
    the 85100th batch, loss is: 0.07218289   
    the 85150th batch, loss is: 0.06898823   
    the 85200th batch, loss is: 0.07909434   
    the 85250th batch, loss is: 0.07370412   
    the 85300th batch, loss is: 0.07227742   
    the 85350th batch, loss is: 0.08326411   
    the 85400th batch, loss is: 0.07700615   
    the 85450th batch, loss is: 0.08805748   
    the 85500th batch, loss is: 0.07896439   
    the 85550th batch, loss is: 0.0824902    
    the 85600th batch, loss is: 0.08550101   
    the 85650th batch, loss is: 0.09841646   
    the 85700th batch, loss is: 0.08041725   
    the 85750th batch, loss is: 0.09064624   
    the 85800th batch, loss is: 0.07469539   
    the 85850th batch, loss is: 0.08118641   
    the 85900th batch, loss is: 0.08333671   
    the 85950th batch, loss is: 0.09091622   
    the 86000th batch, loss is: 0.07699084   
    the 86050th batch, loss is: 0.08525737   
    the 86100th batch, loss is: 0.08198957   
    the 86150th batch, loss is: 0.07674594   
    the 86200th batch, loss is: 0.07038288   
    the 86250th batch, loss is: 0.07590386   
    the 86300th batch, loss is: 0.0764905    
    the 86350th batch, loss is: 0.08726186   
    the 86400th batch, loss is: 0.07808596   
    the 86450th batch, loss is: 0.07703624   
    the 86500th batch, loss is: 0.08096141   
    the 86550th batch, loss is: 0.09218349   
    the 86600th batch, loss is: 0.09501155   
    the 86650th batch, loss is: 0.08247896   
    the 86700th batch, loss is: 0.07115603   
    the 86750th batch, loss is: 0.08035175   
    the 86800th batch, loss is: 0.07707029   
    the 86850th batch, loss is: 0.08869513   
('Epoch: ', 29, 'lr is: [0.001]')
    the 86900th batch, loss is: 0.07977573   
    the 86950th batch, loss is: 0.08134089   
    the 87000th batch, loss is: 0.08648178   
    the 87050th batch, loss is: 0.06337912   
    the 87100th batch, loss is: 0.07921546   
    the 87150th batch, loss is: 0.08444366   
    the 87200th batch, loss is: 0.07705858   
    the 87250th batch, loss is: 0.08057461   
    the 87300th batch, loss is: 0.06666692   
    the 87350th batch, loss is: 0.08176085   
    the 87400th batch, loss is: 0.07862072   
    the 87450th batch, loss is: 0.09397723   
    the 87500th batch, loss is: 0.06692322   
    the 87550th batch, loss is: 0.08114375   
    the 87600th batch, loss is: 0.08646658   
    the 87650th batch, loss is: 0.09684863   
    the 87700th batch, loss is: 0.07594051   
    the 87750th batch, loss is: 0.0915987    
    the 87800th batch, loss is: 0.12209507   
    the 87850th batch, loss is: 0.07830422   
    the 87900th batch, loss is: 0.08072551   
    the 87950th batch, loss is: 0.07904486   
    the 88000th batch, loss is: 0.09204058   
    the 88050th batch, loss is: 0.07876892   
    the 88100th batch, loss is: 0.08359559   
    the 88150th batch, loss is: 0.07475823   
    the 88200th batch, loss is: 0.06850142   
    the 88250th batch, loss is: 0.08217735   
    the 88300th batch, loss is: 0.0797838    
    the 88350th batch, loss is: 0.07651172   
    the 88400th batch, loss is: 0.07771139   
    the 88450th batch, loss is: 0.08669309   
    the 88500th batch, loss is: 0.08140558   
    the 88550th batch, loss is: 0.0831755    
    the 88600th batch, loss is: 0.07503282   
    the 88650th batch, loss is: 0.0817957    
    the 88700th batch, loss is: 0.08522438   
    the 88750th batch, loss is: 0.07280406   
    the 88800th batch, loss is: 0.07498926   
    the 88850th batch, loss is: 0.08044595   
    the 88900th batch, loss is: 0.07341992   
    the 88950th batch, loss is: 0.06947511   
    the 89000th batch, loss is: 0.08352647   
    the 89050th batch, loss is: 0.07832432   
    the 89100th batch, loss is: 0.06931843   
    the 89150th batch, loss is: 0.0718582    
    the 89200th batch, loss is: 0.08775592   
    the 89250th batch, loss is: 0.08384578   
    the 89300th batch, loss is: 0.10551934   
    the 89350th batch, loss is: 0.0949626    
    the 89400th batch, loss is: 0.07755948   
    the 89450th batch, loss is: 0.09182841   
    the 89500th batch, loss is: 0.08454568   
    the 89550th batch, loss is: 0.07871021   
    the 89600th batch, loss is: 0.07793169   
    the 89650th batch, loss is: 0.1100492    
    the 89700th batch, loss is: 0.09159479   
    the 89750th batch, loss is: 0.08009201   
    the 89800th batch, loss is: 0.08014483   
('Epoch: ', 30, 'lr is: [0.001]')
    the 89850th batch, loss is: 0.08572559   
    the 89900th batch, loss is: 0.07392443   
    the 89950th batch, loss is: 0.07429947   
    the 90000th batch, loss is: 0.08172434   
    the 90050th batch, loss is: 0.07784052   
    the 90100th batch, loss is: 0.09433717   
    the 90150th batch, loss is: 0.08165535   
    the 90200th batch, loss is: 0.08187728   
    the 90250th batch, loss is: 0.10455353   
    the 90300th batch, loss is: 0.07864416   
    the 90350th batch, loss is: 0.06582667   
    the 90400th batch, loss is: 0.07051536   
    the 90450th batch, loss is: 0.07731535   
    the 90500th batch, loss is: 0.10224695   
    the 90550th batch, loss is: 0.08519379   
    the 90600th batch, loss is: 0.08380508   
    the 90650th batch, loss is: 0.07102396   
    the 90700th batch, loss is: 0.08306645   
    the 90750th batch, loss is: 0.08154257   
    the 90800th batch, loss is: 0.07355867   
    the 90850th batch, loss is: 0.08319911   
    the 90900th batch, loss is: 0.07794157   
    the 90950th batch, loss is: 0.07657526   
    the 91000th batch, loss is: 0.06800653   
    the 91050th batch, loss is: 0.08388353   
    the 91100th batch, loss is: 0.072537     
    the 91150th batch, loss is: 0.08091195   
    the 91200th batch, loss is: 0.09196832   
    the 91250th batch, loss is: 0.07112487   
    the 91300th batch, loss is: 0.06953687   
    the 91350th batch, loss is: 0.08977533   
    the 91400th batch, loss is: 0.07436919   
    the 91450th batch, loss is: 0.09259062   
    the 91500th batch, loss is: 0.07876889   
    the 91550th batch, loss is: 0.07817473   
    the 91600th batch, loss is: 0.07164141   
    the 91650th batch, loss is: 0.0750256    
    the 91700th batch, loss is: 0.10039204   
    the 91750th batch, loss is: 0.08507687   
    the 91800th batch, loss is: 0.074442     
    the 91850th batch, loss is: 0.06790579   
    the 91900th batch, loss is: 0.07238584   
    the 91950th batch, loss is: 0.09326875   
    the 92000th batch, loss is: 0.08007108   
    the 92050th batch, loss is: 0.08076414   
    the 92100th batch, loss is: 0.08065783   
    the 92150th batch, loss is: 0.08605225   
    the 92200th batch, loss is: 0.07331081   
    the 92250th batch, loss is: 0.07043306   
    the 92300th batch, loss is: 0.08323117   
    the 92350th batch, loss is: 0.07581822   
    the 92400th batch, loss is: 0.09554817   
    the 92450th batch, loss is: 0.08523036   
    the 92500th batch, loss is: 0.0827636    
    the 92550th batch, loss is: 0.08356678   
    the 92600th batch, loss is: 0.07172021   
    the 92650th batch, loss is: 0.08127919   
    the 92700th batch, loss is: 0.08639104   
    the 92750th batch, loss is: 0.07530511   
    the 92800th batch, loss is: 0.06721149   
('Epoch: ', 31, 'lr is: [0.001]')
    the 92850th batch, loss is: 0.07224181   
    the 92900th batch, loss is: 0.07967827   
    the 92950th batch, loss is: 0.07050721   
    the 93000th batch, loss is: 0.08427539   
    the 93050th batch, loss is: 0.07566393   
    the 93100th batch, loss is: 0.07436977   
    the 93150th batch, loss is: 0.09150369   
    the 93200th batch, loss is: 0.0659112    
    the 93250th batch, loss is: 0.06830043   
    the 93300th batch, loss is: 0.06930117   
    the 93350th batch, loss is: 0.08265235   
    the 93400th batch, loss is: 0.06426722   
    the 93450th batch, loss is: 0.08320845   
    the 93500th batch, loss is: 0.07969788   
    the 93550th batch, loss is: 0.06485675   
    the 93600th batch, loss is: 0.06882755   
    the 93650th batch, loss is: 0.07816051   
    the 93700th batch, loss is: 0.09131588   
    the 93750th batch, loss is: 0.06553305   
    the 93800th batch, loss is: 0.07246319   
    the 93850th batch, loss is: 0.0783189    
    the 93900th batch, loss is: 0.08367807   
    the 93950th batch, loss is: 0.06100447   
    the 94000th batch, loss is: 0.06852847   
    the 94050th batch, loss is: 0.07639956   
    the 94100th batch, loss is: 0.07471692   
    the 94150th batch, loss is: 0.08433743   
    the 94200th batch, loss is: 0.06656002   
    the 94250th batch, loss is: 0.07602542   
    the 94300th batch, loss is: 0.08410297   
    the 94350th batch, loss is: 0.05765304   
    the 94400th batch, loss is: 0.07952556   
    the 94450th batch, loss is: 0.07007961   
    the 94500th batch, loss is: 0.07571778   
    the 94550th batch, loss is: 0.0884193    
    the 94600th batch, loss is: 0.07449802   
    the 94650th batch, loss is: 0.06913397   
    the 94700th batch, loss is: 0.07693883   
    the 94750th batch, loss is: 0.07441372   
    the 94800th batch, loss is: 0.07296713   
    the 94850th batch, loss is: 0.09712515   
    the 94900th batch, loss is: 0.08599968   
    the 94950th batch, loss is: 0.07130222   
    the 95000th batch, loss is: 0.06276756   
    the 95050th batch, loss is: 0.06772241   
    the 95100th batch, loss is: 0.08069807   
    the 95150th batch, loss is: 0.08257174   
    the 95200th batch, loss is: 0.08770453   
    the 95250th batch, loss is: 0.07882362   
    the 95300th batch, loss is: 0.07935548   
    the 95350th batch, loss is: 0.07483728   
    the 95400th batch, loss is: 0.08529713   
    the 95450th batch, loss is: 0.07586398   
    the 95500th batch, loss is: 0.10083732   
    the 95550th batch, loss is: 0.07711314   
    the 95600th batch, loss is: 0.09475571   
    the 95650th batch, loss is: 0.06236391   
    the 95700th batch, loss is: 0.08268696   
    the 95750th batch, loss is: 0.10285094   
    the 95800th batch, loss is: 0.09256369   
('Epoch: ', 32, 'lr is: [0.001]')
    the 95850th batch, loss is: 0.07629645   
    the 95900th batch, loss is: 0.06510303   
    the 95950th batch, loss is: 0.07195417   
    the 96000th batch, loss is: 0.08417325   
    the 96050th batch, loss is: 0.0651778    
    the 96100th batch, loss is: 0.07851305   
    the 96150th batch, loss is: 0.07699469   
    the 96200th batch, loss is: 0.07773899   
    the 96250th batch, loss is: 0.0788393    
    the 96300th batch, loss is: 0.06866945   
    the 96350th batch, loss is: 0.07924184   
    the 96400th batch, loss is: 0.08962462   
    the 96450th batch, loss is: 0.07422127   
    the 96500th batch, loss is: 0.06767141   
    the 96550th batch, loss is: 0.07798593   
    the 96600th batch, loss is: 0.06367044   
    the 96650th batch, loss is: 0.07718239   
    the 96700th batch, loss is: 0.8047598    
    the 96750th batch, loss is: 0.07215153   
    the 96800th batch, loss is: 0.0833148    
    the 96850th batch, loss is: 0.07720015   
    the 96900th batch, loss is: 0.07804363   
    the 96950th batch, loss is: 0.07176185   
    the 97000th batch, loss is: 0.08103694   
    the 97050th batch, loss is: 0.0923229    
    the 97100th batch, loss is: 0.08292657   
    the 97150th batch, loss is: 0.08977722   
    the 97200th batch, loss is: 0.07706357   
    the 97250th batch, loss is: 0.0778835    
    the 97300th batch, loss is: 0.07892955   
    the 97350th batch, loss is: 0.07970695   
    the 97400th batch, loss is: 0.06966391   
    the 97450th batch, loss is: 0.07506894   
    the 97500th batch, loss is: 0.06437045   
    the 97550th batch, loss is: 0.0645434    
    the 97600th batch, loss is: 0.06422177   
    the 97650th batch, loss is: 0.06345196   
    the 97700th batch, loss is: 0.07401566   
    the 97750th batch, loss is: 0.09403779   
    the 97800th batch, loss is: 0.08087391   
    the 97850th batch, loss is: 0.06446644   
    the 97900th batch, loss is: 0.06995074   
    the 97950th batch, loss is: 0.08725271   
    the 98000th batch, loss is: 0.07447378   
    the 98050th batch, loss is: 0.07639785   
    the 98100th batch, loss is: 0.07964161   
    the 98150th batch, loss is: 0.10074294   
    the 98200th batch, loss is: 0.07510169   
    the 98250th batch, loss is: 0.07496598   
    the 98300th batch, loss is: 0.05984845   
    the 98350th batch, loss is: 0.07549324   
    the 98400th batch, loss is: 0.07925231   
    the 98450th batch, loss is: 0.07070114   
    the 98500th batch, loss is: 0.0692057    
    the 98550th batch, loss is: 0.08493764   
    the 98600th batch, loss is: 0.07763151   
    the 98650th batch, loss is: 0.07695466   
    the 98700th batch, loss is: 0.08194841   
    the 98750th batch, loss is: 0.07291126   
    the 98800th batch, loss is: 0.08265873   
('Epoch: ', 33, 'lr is: [0.001]')
    the 98850th batch, loss is: 0.08335462   
    the 98900th batch, loss is: 0.07485779   
    the 98950th batch, loss is: 0.07114428   
    the 99000th batch, loss is: 0.07789343   
    the 99050th batch, loss is: 0.07189018   
    the 99100th batch, loss is: 0.0731703    
    the 99150th batch, loss is: 0.07477745   
    the 99200th batch, loss is: 0.0790437    
    the 99250th batch, loss is: 0.06324687   
    the 99300th batch, loss is: 0.06858242   
    the 99350th batch, loss is: 0.06401169   
    the 99400th batch, loss is: 0.0753516    
    the 99450th batch, loss is: 0.0797681    
    the 99500th batch, loss is: 0.06166289   
    the 99550th batch, loss is: 0.0618065    
    the 99600th batch, loss is: 0.06859559   
    the 99650th batch, loss is: 0.08679544   
    the 99700th batch, loss is: 0.09532859   
    the 99750th batch, loss is: 0.07102515   
    the 99800th batch, loss is: 0.08409054   
    the 99850th batch, loss is: 0.07661815   
    the 99900th batch, loss is: 0.09510738   
    the 99950th batch, loss is: 0.06596395   
   the 100000th batch, loss is: 0.08103497   
   the 100050th batch, loss is: 0.07486092   
   the 100100th batch, loss is: 0.06477413   
   the 100150th batch, loss is: 0.06742126   
   the 100200th batch, loss is: 0.07999094   
   the 100250th batch, loss is: 0.06911407   
   the 100300th batch, loss is: 0.07890068   
   the 100350th batch, loss is: 0.06853116   
   the 100400th batch, loss is: 0.06814816   
   the 100450th batch, loss is: 0.06937955   
   the 100500th batch, loss is: 0.08520853   
   the 100550th batch, loss is: 0.07967963   
   the 100600th batch, loss is: 0.07515325   
   the 100650th batch, loss is: 0.07665018   
   the 100700th batch, loss is: 0.07703459   
   the 100750th batch, loss is: 0.19016755   
   the 100800th batch, loss is: 0.08222625   
   the 100850th batch, loss is: 0.07189831   
   the 100900th batch, loss is: 0.06775058   
   the 100950th batch, loss is: 0.07501944   
   the 101000th batch, loss is: 0.06658627   
   the 101050th batch, loss is: 0.06315339   
   the 101100th batch, loss is: 0.08935827   
   the 101150th batch, loss is: 0.07570862   
   the 101200th batch, loss is: 0.07028972   
   the 101250th batch, loss is: 0.0764349    
   the 101300th batch, loss is: 0.06880996   
   the 101350th batch, loss is: 0.07754192   
   the 101400th batch, loss is: 0.06542838   
   the 101450th batch, loss is: 0.06880037   
   the 101500th batch, loss is: 0.0669399    
   the 101550th batch, loss is: 0.06918      
   the 101600th batch, loss is: 0.08378866   
   the 101650th batch, loss is: 0.07105203   
   the 101700th batch, loss is: 0.06434977   
   the 101750th batch, loss is: 0.09254169   
   the 101800th batch, loss is: 0.07853907   
('Epoch: ', 34, 'lr is: [0.001]')
   the 101850th batch, loss is: 0.07817744   
   the 101900th batch, loss is: 0.06474322   
   the 101950th batch, loss is: 0.07068654   
   the 102000th batch, loss is: 0.07793644   
   the 102050th batch, loss is: 0.0523223    
   the 102100th batch, loss is: 0.08043154   
   the 102150th batch, loss is: 0.0758779    
   the 102200th batch, loss is: 0.06274761   
   the 102250th batch, loss is: 0.07179689   
   the 102300th batch, loss is: 0.07959616   
   the 102350th batch, loss is: 0.06243336   
   the 102400th batch, loss is: 0.07826973   
   the 102450th batch, loss is: 0.07519183   
   the 102500th batch, loss is: 0.06398783   
   the 102550th batch, loss is: 0.07548148   
   the 102600th batch, loss is: 0.06722862   
   the 102650th batch, loss is: 0.08029144   
   the 102700th batch, loss is: 0.07379689   
   the 102750th batch, loss is: 0.0826977    
   the 102800th batch, loss is: 0.08509827   
   the 102850th batch, loss is: 0.07343911   
   the 102900th batch, loss is: 0.06321637   
   the 102950th batch, loss is: 0.06625369   
   the 103000th batch, loss is: 0.07525525   
   the 103050th batch, loss is: 0.0613948    
   the 103100th batch, loss is: 0.06545875   
   the 103150th batch, loss is: 0.07276314   
   the 103200th batch, loss is: 0.06626272   
   the 103250th batch, loss is: 0.06994941   
   the 103300th batch, loss is: 0.072106     
   the 103350th batch, loss is: 0.06760198   
   the 103400th batch, loss is: 0.06515872   
   the 103450th batch, loss is: 0.08073065   
   the 103500th batch, loss is: 0.07972694   
   the 103550th batch, loss is: 0.06432303   
   the 103600th batch, loss is: 0.0866728    
   the 103650th batch, loss is: 0.06494703   
   the 103700th batch, loss is: 0.08132999   
   the 103750th batch, loss is: 0.0681011    
   the 103800th batch, loss is: 0.07216018   
   the 103850th batch, loss is: 0.08851136   
   the 103900th batch, loss is: 0.06366631   
   the 103950th batch, loss is: 0.08791471   
   the 104000th batch, loss is: 0.07044612   
   the 104050th batch, loss is: 0.06999163   
   the 104100th batch, loss is: 0.08440148   
   the 104150th batch, loss is: 0.06914188   
   the 104200th batch, loss is: 0.07271859   
   the 104250th batch, loss is: 0.07316442   
   the 104300th batch, loss is: 0.08229439   
   the 104350th batch, loss is: 0.07491932   
   the 104400th batch, loss is: 0.11288889   
   the 104450th batch, loss is: 0.08409964   
   the 104500th batch, loss is: 0.07439204   
   the 104550th batch, loss is: 0.07423158   
   the 104600th batch, loss is: 0.08117557   
   the 104650th batch, loss is: 0.07479739   
   the 104700th batch, loss is: 0.0719979    
   the 104750th batch, loss is: 0.07691881   
   the 104800th batch, loss is: 0.09315003   
('Epoch: ', 35, 'lr is: [0.001]')
   the 104850th batch, loss is: 0.06084238   
   the 104900th batch, loss is: 0.05973572   
   the 104950th batch, loss is: 0.06008613   
   the 105000th batch, loss is: 0.07880086   
   the 105050th batch, loss is: 0.11078968   
   the 105100th batch, loss is: 0.06623086   
   the 105150th batch, loss is: 0.06853519   
   the 105200th batch, loss is: 0.06359387   
   the 105250th batch, loss is: 0.06628007   
   the 105300th batch, loss is: 0.05389535   
   the 105350th batch, loss is: 0.06692369   
   the 105400th batch, loss is: 0.06303249   
   the 105450th batch, loss is: 0.06500817   
   the 105500th batch, loss is: 0.05603631   
   the 105550th batch, loss is: 0.06807002   
   the 105600th batch, loss is: 0.07883481   
   the 105650th batch, loss is: 0.07433775   
   the 105700th batch, loss is: 0.07974564   
   the 105750th batch, loss is: 0.05951213   
   the 105800th batch, loss is: 0.07319253   
   the 105850th batch, loss is: 0.06564699   
   the 105900th batch, loss is: 0.08077849   
   the 105950th batch, loss is: 0.07640168   
   the 106000th batch, loss is: 0.06947087   
   the 106050th batch, loss is: 0.06955838   
   the 106100th batch, loss is: 0.08513116   
   the 106150th batch, loss is: 0.07461798   
   the 106200th batch, loss is: 0.05827151   
   the 106250th batch, loss is: 0.06830292   
   the 106300th batch, loss is: 0.07192992   
   the 106350th batch, loss is: 0.06439582   
   the 106400th batch, loss is: 0.06937768   
   the 106450th batch, loss is: 0.0722347    
   the 106500th batch, loss is: 0.09461447   
   the 106550th batch, loss is: 0.07081012   
   the 106600th batch, loss is: 0.07164769   
   the 106650th batch, loss is: 0.06560642   
   the 106700th batch, loss is: 0.07207491   
   the 106750th batch, loss is: 0.06882682   
   the 106800th batch, loss is: 0.09061032   
   the 106850th batch, loss is: 0.06756167   
   the 106900th batch, loss is: 0.07205132   
   the 106950th batch, loss is: 0.06807959   
   the 107000th batch, loss is: 0.07606751   
   the 107050th batch, loss is: 0.07925732   
   the 107100th batch, loss is: 0.07102725   
   the 107150th batch, loss is: 0.06935787   
   the 107200th batch, loss is: 0.07994308   
   the 107250th batch, loss is: 0.06546877   
   the 107300th batch, loss is: 0.08231844   
   the 107350th batch, loss is: 0.07339761   
   the 107400th batch, loss is: 0.85768056   
   the 107450th batch, loss is: 0.07207045   
   the 107500th batch, loss is: 0.07175426   
   the 107550th batch, loss is: 0.08674382   
   the 107600th batch, loss is: 0.07083333   
   the 107650th batch, loss is: 0.08912504   
   the 107700th batch, loss is: 0.07547383   
   the 107750th batch, loss is: 0.09159113   
   the 107800th batch, loss is: 0.08355217   
('Epoch: ', 36, 'lr is: [0.001]')
   the 107850th batch, loss is: 0.07845379   
   the 107900th batch, loss is: 0.07156783   
   the 107950th batch, loss is: 0.06978446   
   the 108000th batch, loss is: 0.07422995   
   the 108050th batch, loss is: 0.07251054   
   the 108100th batch, loss is: 0.06422883   
   the 108150th batch, loss is: 0.07569565   
   the 108200th batch, loss is: 0.06792391   
   the 108250th batch, loss is: 0.07274302   
   the 108300th batch, loss is: 0.06166572   
   the 108350th batch, loss is: 0.06762818   
   the 108400th batch, loss is: 0.06263699   
   the 108450th batch, loss is: 0.07610442   
   the 108500th batch, loss is: 0.05800935   
   the 108550th batch, loss is: 0.06765818   
   the 108600th batch, loss is: 0.05640715   
   the 108650th batch, loss is: 0.07606503   
   the 108700th batch, loss is: 0.06913796   
   the 108750th batch, loss is: 0.08390179   
   the 108800th batch, loss is: 0.08445113   
   the 108850th batch, loss is: 0.07231143   
   the 108900th batch, loss is: 0.06783369   
   the 108950th batch, loss is: 0.08843107   
   the 109000th batch, loss is: 0.07603813   
   the 109050th batch, loss is: 0.06421466   
   the 109100th batch, loss is: 0.08634638   
   the 109150th batch, loss is: 0.06979813   
   the 109200th batch, loss is: 0.07981956   
   the 109250th batch, loss is: 0.08110809   
   the 109300th batch, loss is: 0.08450432   
   the 109350th batch, loss is: 0.07352038   
   the 109400th batch, loss is: 0.10220954   
   the 109450th batch, loss is: 0.08118122   
   the 109500th batch, loss is: 0.06523134   
   the 109550th batch, loss is: 0.08531429   
   the 109600th batch, loss is: 0.09129264   
   the 109650th batch, loss is: 0.08775213   
   the 109700th batch, loss is: 0.06715907   
   the 109750th batch, loss is: 0.07167558   
   the 109800th batch, loss is: 0.07823113   
   the 109850th batch, loss is: 0.06889716   
   the 109900th batch, loss is: 0.0669934    
   the 109950th batch, loss is: 0.07846699   
   the 110000th batch, loss is: 0.10430822   
   the 110050th batch, loss is: 0.07681388   
   the 110100th batch, loss is: 0.0830468    
   the 110150th batch, loss is: 0.07166845   
   the 110200th batch, loss is: 0.06984927   
   the 110250th batch, loss is: 0.06992198   
   the 110300th batch, loss is: 0.07390986   
   the 110350th batch, loss is: 0.07487451   
   the 110400th batch, loss is: 0.12654915   
   the 110450th batch, loss is: 0.0793433    
   the 110500th batch, loss is: 0.07524593   
   the 110550th batch, loss is: 0.06706151   
   the 110600th batch, loss is: 0.07044236   
   the 110650th batch, loss is: 0.07450726   
   the 110700th batch, loss is: 0.08730961   
   the 110750th batch, loss is: 0.07094567   
   the 110800th batch, loss is: 0.07203416   
('Epoch: ', 37, 'lr is: [0.001]')
   the 110850th batch, loss is: 0.06792102   
   the 110900th batch, loss is: 0.072795     
   the 110950th batch, loss is: 0.0700267    
   the 111000th batch, loss is: 0.09170518   
   the 111050th batch, loss is: 0.0800736    
   the 111100th batch, loss is: 0.06572272   
   the 111150th batch, loss is: 0.08600596   
   the 111200th batch, loss is: 0.07692624   
   the 111250th batch, loss is: 0.06866028   
   the 111300th batch, loss is: 0.07460785   
   the 111350th batch, loss is: 0.07649069   
   the 111400th batch, loss is: 0.069536     
   the 111450th batch, loss is: 0.07964629   
   the 111500th batch, loss is: 0.06837894   
   the 111550th batch, loss is: 0.06570486   
   the 111600th batch, loss is: 0.07507392   
   the 111650th batch, loss is: 0.07651786   
   the 111700th batch, loss is: 0.06554431   
   the 111750th batch, loss is: 0.06210167   
   the 111800th batch, loss is: 0.06550116   
   the 111850th batch, loss is: 0.07484321   
   the 111900th batch, loss is: 0.07932081   
   the 111950th batch, loss is: 0.08446825   
   the 112000th batch, loss is: 0.06916235   
   the 112050th batch, loss is: 0.07101402   
   the 112100th batch, loss is: 0.06505716   
   the 112150th batch, loss is: 0.07975224   
   the 112200th batch, loss is: 0.07689684   
   the 112250th batch, loss is: 0.0678345    
   the 112300th batch, loss is: 0.06577289   
   the 112350th batch, loss is: 0.08167852   
   the 112400th batch, loss is: 0.09717033   
   the 112450th batch, loss is: 0.07347302   
   the 112500th batch, loss is: 0.08266949   
   the 112550th batch, loss is: 0.08204379   
   the 112600th batch, loss is: 0.06992433   
   the 112650th batch, loss is: 0.05890459   
   the 112700th batch, loss is: 0.0728958    
   the 112750th batch, loss is: 0.05826496   
   the 112800th batch, loss is: 0.07434663   
   the 112850th batch, loss is: 0.07383914   
   the 112900th batch, loss is: 0.08150489   
   the 112950th batch, loss is: 0.08476302   
   the 113000th batch, loss is: 0.10119745   
   the 113050th batch, loss is: 0.0797125    
   the 113100th batch, loss is: 0.08689726   
   the 113150th batch, loss is: 0.06520247   
   the 113200th batch, loss is: 0.0736094    
   the 113250th batch, loss is: 0.09273471   
   the 113300th batch, loss is: 0.07816073   
   the 113350th batch, loss is: 0.08890733   
   the 113400th batch, loss is: 0.07217917   
   the 113450th batch, loss is: 0.11187815   
   the 113500th batch, loss is: 0.08039623   
   the 113550th batch, loss is: 0.07962631   
   the 113600th batch, loss is: 0.07368322   
   the 113650th batch, loss is: 0.08112403   
   the 113700th batch, loss is: 0.07984579   
   the 113750th batch, loss is: 0.08556415   
   the 113800th batch, loss is: 0.06867932   
('Epoch: ', 38, 'lr is: [0.001]')
   the 113850th batch, loss is: 0.07733276   
   the 113900th batch, loss is: 0.0685662    
   the 113950th batch, loss is: 0.06635287   
   the 114000th batch, loss is: 0.08407903   
   the 114050th batch, loss is: 0.06323995   
   the 114100th batch, loss is: 0.06526699   
   the 114150th batch, loss is: 0.07504044   
   the 114200th batch, loss is: 0.08025441   
   the 114250th batch, loss is: 0.10969055   
   the 114300th batch, loss is: 0.07107808   
   the 114350th batch, loss is: 0.07348769   
   the 114400th batch, loss is: 0.07115849   
   the 114450th batch, loss is: 0.07191554   
   the 114500th batch, loss is: 0.07432444   
   the 114550th batch, loss is: 0.08413385   
   the 114600th batch, loss is: 0.0752796    
   the 114650th batch, loss is: 0.07615407   
   the 114700th batch, loss is: 0.08200455   
   the 114750th batch, loss is: 0.09680694   
   the 114800th batch, loss is: 0.07920919   
   the 114850th batch, loss is: 0.08978673   
   the 114900th batch, loss is: 0.08307052   
   the 114950th batch, loss is: 0.07455596   
   the 115000th batch, loss is: 0.06848735   
   the 115050th batch, loss is: 0.06993747   
   the 115100th batch, loss is: 0.06771838   
   the 115150th batch, loss is: 0.06563544   
   the 115200th batch, loss is: 0.06410056   
   the 115250th batch, loss is: 0.06799275   
   the 115300th batch, loss is: 0.07663131   
   the 115350th batch, loss is: 0.07230432   
   the 115400th batch, loss is: 0.07509568   
   the 115450th batch, loss is: 0.06711479   
   the 115500th batch, loss is: 0.07637075   
   the 115550th batch, loss is: 0.08443221   
   the 115600th batch, loss is: 0.06790718   
   the 115650th batch, loss is: 0.07468131   
   the 115700th batch, loss is: 0.07916259   
   the 115750th batch, loss is: 0.07941531   
   the 115800th batch, loss is: 0.07928996   
   the 115850th batch, loss is: 0.07206099   
   the 115900th batch, loss is: 0.08091335   
   the 115950th batch, loss is: 0.07641143   
   the 116000th batch, loss is: 0.07696936   
   the 116050th batch, loss is: 0.0754292    
   the 116100th batch, loss is: 0.095411     
   the 116150th batch, loss is: 0.06416512   
   the 116200th batch, loss is: 0.08679418   
   the 116250th batch, loss is: 0.08039742   
   the 116300th batch, loss is: 0.09047998   
   the 116350th batch, loss is: 0.06870145   
   the 116400th batch, loss is: 0.07114496   
   the 116450th batch, loss is: 0.08027671   
   the 116500th batch, loss is: 0.07667399   
   the 116550th batch, loss is: 0.08230222   
   the 116600th batch, loss is: 0.08303671   
   the 116650th batch, loss is: 0.07517793   
   the 116700th batch, loss is: 0.06438542   
   the 116750th batch, loss is: 0.07254594   
   the 116800th batch, loss is: 0.08397353   
('Epoch: ', 39, 'lr is: [0.001]')
   the 116850th batch, loss is: 0.08545639   
   the 116900th batch, loss is: 0.08881659   
   the 116950th batch, loss is: 0.06128985   
   the 117000th batch, loss is: 0.06209263   
   the 117050th batch, loss is: 0.0687395    
   the 117100th batch, loss is: 0.07437564   
   the 117150th batch, loss is: 0.08477055   
   the 117200th batch, loss is: 0.07232609   
   the 117250th batch, loss is: 0.07433843   
   the 117300th batch, loss is: 0.08043738   
   the 117350th batch, loss is: 0.06255759   
   the 117400th batch, loss is: 0.07247906   
   the 117450th batch, loss is: 0.07987488   
   the 117500th batch, loss is: 0.06659196   
   the 117550th batch, loss is: 0.06782533   
   the 117600th batch, loss is: 0.07550209   
   the 117650th batch, loss is: 0.0757146    
   the 117700th batch, loss is: 0.07060011   
   the 117750th batch, loss is: 0.07198292   
   the 117800th batch, loss is: 0.06291243   
   the 117850th batch, loss is: 0.07142765   
   the 117900th batch, loss is: 0.09112789   
   the 117950th batch, loss is: 0.07092042   
   the 118000th batch, loss is: 0.07561091   
   the 118050th batch, loss is: 0.0724457    
   the 118100th batch, loss is: 0.08894948   
   the 118150th batch, loss is: 0.06575295   
   the 118200th batch, loss is: 0.08242539   
   the 118250th batch, loss is: 0.06912389   
   the 118300th batch, loss is: 0.07518166   
   the 118350th batch, loss is: 0.06964462   
   the 118400th batch, loss is: 0.06800885   
   the 118450th batch, loss is: 0.06918384   
   the 118500th batch, loss is: 0.0660042    
   the 118550th batch, loss is: 0.07973551   
   the 118600th batch, loss is: 0.07741145   
   the 118650th batch, loss is: 0.08528861   
   the 118700th batch, loss is: 0.09208029   
   the 118750th batch, loss is: 0.07886434   
   the 118800th batch, loss is: 0.06970192   
   the 118850th batch, loss is: 0.08297381   
   the 118900th batch, loss is: 0.08071998   
   the 118950th batch, loss is: 0.08360982   
   the 119000th batch, loss is: 0.07170379   
   the 119050th batch, loss is: 0.06983876   
   the 119100th batch, loss is: 0.07188913   
   the 119150th batch, loss is: 0.06638697   
   the 119200th batch, loss is: 0.07704762   
   the 119250th batch, loss is: 0.08182347   
   the 119300th batch, loss is: 0.07920285   
   the 119350th batch, loss is: 0.07430482   
   the 119400th batch, loss is: 0.08325353   
   the 119450th batch, loss is: 0.06835068   
   the 119500th batch, loss is: 0.09471159   
   the 119550th batch, loss is: 0.07045222   
   the 119600th batch, loss is: 0.09007061   
   the 119650th batch, loss is: 0.07549974   
   the 119700th batch, loss is: 0.08331142   
   the 119750th batch, loss is: 0.08641234   
('Epoch: ', 40, 'lr is: [0.001]')
   the 119800th batch, loss is: 0.08877224   
   the 119850th batch, loss is: 0.07524235   
   the 119900th batch, loss is: 0.05614609   
   the 119950th batch, loss is: 0.07537863   
   the 120000th batch, loss is: 0.05874063   
   the 120050th batch, loss is: 0.06463199   
   the 120100th batch, loss is: 0.06798535   
   the 120150th batch, loss is: 0.07027515   
   the 120200th batch, loss is: 0.08219729   
   the 120250th batch, loss is: 0.06332321   
   the 120300th batch, loss is: 0.07310271   
   the 120350th batch, loss is: 0.06995736   
   the 120400th batch, loss is: 0.06799322   
   the 120450th batch, loss is: 0.07460403   
   the 120500th batch, loss is: 0.06457537   
   the 120550th batch, loss is: 0.07357109   
   the 120600th batch, loss is: 0.06977005   
   the 120650th batch, loss is: 0.07939152   
   the 120700th batch, loss is: 0.06924253   
   the 120750th batch, loss is: 0.06988433   
   the 120800th batch, loss is: 0.06510472   
   the 120850th batch, loss is: 0.05375268   
   the 120900th batch, loss is: 0.07173377   
   the 120950th batch, loss is: 0.05837497   
   the 121000th batch, loss is: 0.06694888   
   the 121050th batch, loss is: 0.06839336   
   the 121100th batch, loss is: 0.07645433   
   the 121150th batch, loss is: 0.08959753   
   the 121200th batch, loss is: 0.06572431   
   the 121250th batch, loss is: 0.06370906   
   the 121300th batch, loss is: 0.06902305   
   the 121350th batch, loss is: 0.06638394   
   the 121400th batch, loss is: 0.07811248   
   the 121450th batch, loss is: 0.06813482   
   the 121500th batch, loss is: 0.08415147   
   the 121550th batch, loss is: 0.06992596   
   the 121600th batch, loss is: 0.07017589   
   the 121650th batch, loss is: 0.06444134   
   the 121700th batch, loss is: 0.06923903   
   the 121750th batch, loss is: 0.06587732   
   the 121800th batch, loss is: 0.08579649   
   the 121850th batch, loss is: 0.06008766   
   the 121900th batch, loss is: 0.09153347   
   the 121950th batch, loss is: 0.07641062   
   the 122000th batch, loss is: 0.0766729    
   the 122050th batch, loss is: 0.07229426   
   the 122100th batch, loss is: 0.06843169   
   the 122150th batch, loss is: 0.08850767   
   the 122200th batch, loss is: 0.07542524   
   the 122250th batch, loss is: 0.08143501   
   the 122300th batch, loss is: 0.06899112   
   the 122350th batch, loss is: 0.07089003   
   the 122400th batch, loss is: 0.07020751   
   the 122450th batch, loss is: 0.07859956   
   the 122500th batch, loss is: 0.07595719   
   the 122550th batch, loss is: 0.06864056   
   the 122600th batch, loss is: 0.08236262   
   the 122650th batch, loss is: 0.06748836   
   the 122700th batch, loss is: 0.07152089   
   the 122750th batch, loss is: 0.09476708   
('Epoch: ', 41, 'lr is: [0.001]')
   the 122800th batch, loss is: 0.06195084   
   the 122850th batch, loss is: 0.09512198   
   the 122900th batch, loss is: 0.07222329   
   the 122950th batch, loss is: 0.06698297   
   the 123000th batch, loss is: 0.07385617   
   the 123050th batch, loss is: 0.06873867   
   the 123100th batch, loss is: 0.07917219   
   the 123150th batch, loss is: 0.08089831   
   the 123200th batch, loss is: 0.07135609   
   the 123250th batch, loss is: 0.06800583   
   the 123300th batch, loss is: 0.07260994   
   the 123350th batch, loss is: 0.06113995   
   the 123400th batch, loss is: 0.08500487   
   the 123450th batch, loss is: 0.06938381   
   the 123500th batch, loss is: 0.08885083   
   the 123550th batch, loss is: 0.07987222   
   the 123600th batch, loss is: 0.05976618   
   the 123650th batch, loss is: 0.06872822   
   the 123700th batch, loss is: 0.06858107   
   the 123750th batch, loss is: 0.06145857   
   the 123800th batch, loss is: 0.07780078   
   the 123850th batch, loss is: 0.07238512   
   the 123900th batch, loss is: 0.07137243   
   the 123950th batch, loss is: 0.06590927   
   the 124000th batch, loss is: 0.09019596   
   the 124050th batch, loss is: 0.07038421   
   the 124100th batch, loss is: 0.06323598   
   the 124150th batch, loss is: 0.07414328   
   the 124200th batch, loss is: 0.07502025   
   the 124250th batch, loss is: 0.06911439   
   the 124300th batch, loss is: 0.06759535   
   the 124350th batch, loss is: 0.06550759   
   the 124400th batch, loss is: 0.07220612   
   the 124450th batch, loss is: 0.07090903   
   the 124500th batch, loss is: 0.06221178   
   the 124550th batch, loss is: 0.06719298   
   the 124600th batch, loss is: 0.08833236   
   the 124650th batch, loss is: 0.07796931   
   the 124700th batch, loss is: 0.07972815   
   the 124750th batch, loss is: 0.06789544   
   the 124800th batch, loss is: 0.07717235   
   the 124850th batch, loss is: 0.07823884   
   the 124900th batch, loss is: 0.08180535   
   the 124950th batch, loss is: 0.06511016   
   the 125000th batch, loss is: 0.06593176   
   the 125050th batch, loss is: 0.07028863   
   the 125100th batch, loss is: 0.0745114    
   the 125150th batch, loss is: 0.07324916   
   the 125200th batch, loss is: 0.07163934   
   the 125250th batch, loss is: 0.06940313   
   the 125300th batch, loss is: 0.0734507    
   the 125350th batch, loss is: 0.07446755   
   the 125400th batch, loss is: 0.07865741   
   the 125450th batch, loss is: 0.07079747   
   the 125500th batch, loss is: 0.06362006   
   the 125550th batch, loss is: 0.08405751   
   the 125600th batch, loss is: 0.08049919   
   the 125650th batch, loss is: 0.06618504   
   the 125700th batch, loss is: 0.08342364   
   the 125750th batch, loss is: 0.07602479   
('Epoch: ', 42, 'lr is: [0.001]')
   the 125800th batch, loss is: 0.068441     
   the 125850th batch, loss is: 0.0771035    
   the 125900th batch, loss is: 0.11790781   
   the 125950th batch, loss is: 0.07521717   
   the 126000th batch, loss is: 0.06560135   
   the 126050th batch, loss is: 0.06607183   
   the 126100th batch, loss is: 0.07258859   
   the 126150th batch, loss is: 0.0963355    
   the 126200th batch, loss is: 0.0664721    
   the 126250th batch, loss is: 0.07234824   
   the 126300th batch, loss is: 0.07957648   
   the 126350th batch, loss is: 0.07089217   
   the 126400th batch, loss is: 0.06002954   
   the 126450th batch, loss is: 0.07901679   
   the 126500th batch, loss is: 0.07091697   
   the 126550th batch, loss is: 0.08530861   
   the 126600th batch, loss is: 0.07002358   
   the 126650th batch, loss is: 0.06075712   
   the 126700th batch, loss is: 0.07004569   
   the 126750th batch, loss is: 0.07166658   
   the 126800th batch, loss is: 0.07637957   
   the 126850th batch, loss is: 0.07545885   
   the 126900th batch, loss is: 0.08611293   
   the 126950th batch, loss is: 0.06202719   
   the 127000th batch, loss is: 0.0788504    
   the 127050th batch, loss is: 0.05788258   
   the 127100th batch, loss is: 0.07978591   
   the 127150th batch, loss is: 0.06431811   
   the 127200th batch, loss is: 0.07690349   
   the 127250th batch, loss is: 0.06880708   
   the 127300th batch, loss is: 0.07325739   
   the 127350th batch, loss is: 0.08435571   
   the 127400th batch, loss is: 0.06835462   
   the 127450th batch, loss is: 0.06524147   
   the 127500th batch, loss is: 0.09370036   
   the 127550th batch, loss is: 0.08339136   
   the 127600th batch, loss is: 0.07190745   
   the 127650th batch, loss is: 0.0650923    
   the 127700th batch, loss is: 0.06357005   
   the 127750th batch, loss is: 0.07341029   
   the 127800th batch, loss is: 0.07454588   
   the 127850th batch, loss is: 0.08146597   
   the 127900th batch, loss is: 0.06628118   
   the 127950th batch, loss is: 0.08176817   
   the 128000th batch, loss is: 0.06725486   
   the 128050th batch, loss is: 0.0662986    
   the 128100th batch, loss is: 0.07693622   
   the 128150th batch, loss is: 0.0694796    
   the 128200th batch, loss is: 0.07655077   
   the 128250th batch, loss is: 0.08175949   
   the 128300th batch, loss is: 0.07047535   
   the 128350th batch, loss is: 0.07415206   
   the 128400th batch, loss is: 0.08358467   
   the 128450th batch, loss is: 0.08135141   
   the 128500th batch, loss is: 0.07721817   
   the 128550th batch, loss is: 0.07367965   
   the 128600th batch, loss is: 0.07172026   
   the 128650th batch, loss is: 0.09000237   
   the 128700th batch, loss is: 0.07528117   
   the 128750th batch, loss is: 0.07098068   
('Epoch: ', 43, 'lr is: [0.001]')
   the 128800th batch, loss is: 0.06973229   
   the 128850th batch, loss is: 0.07201711   
   the 128900th batch, loss is: 0.05801687   
   the 128950th batch, loss is: 0.06537735   
   the 129000th batch, loss is: 0.08098577   
   the 129050th batch, loss is: 0.07319619   
   the 129100th batch, loss is: 0.06766228   
   the 129150th batch, loss is: 0.06722327   
   the 129200th batch, loss is: 0.06713936   
   the 129250th batch, loss is: 0.07105251   
   the 129300th batch, loss is: 0.09242755   
   the 129350th batch, loss is: 0.07108511   
   the 129400th batch, loss is: 0.07222427   
   the 129450th batch, loss is: 0.06944395   
   the 129500th batch, loss is: 0.07462642   
   the 129550th batch, loss is: 0.06663067   
   the 129600th batch, loss is: 0.06892994   
   the 129650th batch, loss is: 0.06609316   
   the 129700th batch, loss is: 0.07803601   
   the 129750th batch, loss is: 0.06709497   
   the 129800th batch, loss is: 0.06816344   
   the 129850th batch, loss is: 0.05986187   
   the 129900th batch, loss is: 0.06671683   
   the 129950th batch, loss is: 0.06366813   
   the 130000th batch, loss is: 0.08629774   
   the 130050th batch, loss is: 0.08626072   
   the 130100th batch, loss is: 0.08930224   
   the 130150th batch, loss is: 0.08605673   
   the 130200th batch, loss is: 0.07100431   
   the 130250th batch, loss is: 0.09655078   
   the 130300th batch, loss is: 0.05863058   
   the 130350th batch, loss is: 0.08772621   
   the 130400th batch, loss is: 0.07379819   
   the 130450th batch, loss is: 0.07024483   
   the 130500th batch, loss is: 0.07051221   
   the 130550th batch, loss is: 0.07052454   
   the 130600th batch, loss is: 0.07448579   
   the 130650th batch, loss is: 0.08245119   
   the 130700th batch, loss is: 0.07020628   
   the 130750th batch, loss is: 0.06684674   
   the 130800th batch, loss is: 0.07010975   
   the 130850th batch, loss is: 0.05929182   
   the 130900th batch, loss is: 0.07762597   
   the 130950th batch, loss is: 0.09475052   
   the 131000th batch, loss is: 0.08539139   
   the 131050th batch, loss is: 0.09127337   
   the 131100th batch, loss is: 0.07761435   
   the 131150th batch, loss is: 0.08128036   
   the 131200th batch, loss is: 0.1004385    
   the 131250th batch, loss is: 0.0843697    
   the 131300th batch, loss is: 0.07959469   
   the 131350th batch, loss is: 0.07136339   
   the 131400th batch, loss is: 0.06620959   
   the 131450th batch, loss is: 0.07275596   
   the 131500th batch, loss is: 0.07920691   
   the 131550th batch, loss is: 0.08017725   
   the 131600th batch, loss is: 0.08624353   
   the 131650th batch, loss is: 0.079654     
   the 131700th batch, loss is: 0.07811616   
   the 131750th batch, loss is: 0.07447565   
('Epoch: ', 44, 'lr is: [0.001]')
   the 131800th batch, loss is: 0.07748301   
   the 131850th batch, loss is: 0.07488973   
   the 131900th batch, loss is: 0.08554489   
   the 131950th batch, loss is: 0.07626171   
   the 132000th batch, loss is: 0.08350907   
   the 132050th batch, loss is: 0.06561475   
   the 132100th batch, loss is: 0.06896877   
   the 132150th batch, loss is: 0.06631713   
   the 132200th batch, loss is: 0.07117226   
   the 132250th batch, loss is: 0.07472321   
   the 132300th batch, loss is: 0.06974883   
   the 132350th batch, loss is: 0.07576826   
   the 132400th batch, loss is: 0.06722507   
   the 132450th batch, loss is: 0.10164242   
   the 132500th batch, loss is: 0.0553172    
   the 132550th batch, loss is: 0.07970013   
   the 132600th batch, loss is: 0.07095432   
   the 132650th batch, loss is: 0.07149342   
   the 132700th batch, loss is: 0.07301096   
   the 132750th batch, loss is: 0.06771631   
   the 132800th batch, loss is: 0.07692737   
   the 132850th batch, loss is: 0.06092336   
   the 132900th batch, loss is: 0.08608108   
   the 132950th batch, loss is: 0.07794756   
   the 133000th batch, loss is: 0.07899214   
   the 133050th batch, loss is: 0.06328685   
   the 133100th batch, loss is: 0.07139388   
   the 133150th batch, loss is: 0.07230826   
   the 133200th batch, loss is: 0.08092939   
   the 133250th batch, loss is: 0.06873975   
   the 133300th batch, loss is: 0.0782666    
   the 133350th batch, loss is: 0.07490232   
   the 133400th batch, loss is: 0.06270286   
   the 133450th batch, loss is: 0.06206542   
   the 133500th batch, loss is: 0.08614381   
   the 133550th batch, loss is: 0.07116187   
   the 133600th batch, loss is: 0.06215087   
   the 133650th batch, loss is: 0.09567234   
   the 133700th batch, loss is: 0.0947007    
   the 133750th batch, loss is: 0.08941708   
   the 133800th batch, loss is: 0.06548846   
   the 133850th batch, loss is: 0.06874569   
   the 133900th batch, loss is: 0.07704689   
   the 133950th batch, loss is: 0.10370176   
   the 134000th batch, loss is: 0.07579961   
   the 134050th batch, loss is: 0.0875838    
   the 134100th batch, loss is: 0.08315044   
   the 134150th batch, loss is: 0.09091099   
   the 134200th batch, loss is: 0.08065271   
   the 134250th batch, loss is: 0.07815578   
   the 134300th batch, loss is: 0.07341925   
   the 134350th batch, loss is: 0.07521471   
   the 134400th batch, loss is: 0.07960396   
   the 134450th batch, loss is: 0.08482744   
   the 134500th batch, loss is: 0.0715697    
   the 134550th batch, loss is: 0.08237763   
   the 134600th batch, loss is: 0.07715578   
   the 134650th batch, loss is: 0.09530312   
   the 134700th batch, loss is: 0.0776986    
   the 134750th batch, loss is: 0.07817008   
('Epoch: ', 45, 'lr is: [0.001]')
   the 134800th batch, loss is: 0.08157052   
   the 134850th batch, loss is: 0.07180164   
   the 134900th batch, loss is: 0.07668398   
   the 134950th batch, loss is: 0.07330608   
   the 135000th batch, loss is: 0.07107984   
   the 135050th batch, loss is: 0.0786027    
   the 135100th batch, loss is: 0.09397721   
   the 135150th batch, loss is: 0.0653336    
   the 135200th batch, loss is: 0.06860096   
   the 135250th batch, loss is: 0.07864997   
   the 135300th batch, loss is: 0.07011892   
   the 135350th batch, loss is: 0.06442114   
   the 135400th batch, loss is: 0.07508619   
   the 135450th batch, loss is: 0.07986992   
   the 135500th batch, loss is: 0.07126274   
   the 135550th batch, loss is: 0.06315575   
   the 135600th batch, loss is: 0.07719783   
   the 135650th batch, loss is: 0.06858935   
   the 135700th batch, loss is: 0.06945298   
   the 135750th batch, loss is: 0.08994339   
   the 135800th batch, loss is: 0.11480649   
   the 135850th batch, loss is: 0.07566395   
   the 135900th batch, loss is: 0.06697024   
   the 135950th batch, loss is: 0.07259716   
   the 136000th batch, loss is: 0.06070522   
   the 136050th batch, loss is: 0.06313752   
   the 136100th batch, loss is: 0.07047141   
   the 136150th batch, loss is: 0.070415     
   the 136200th batch, loss is: 0.06883026   
   the 136250th batch, loss is: 0.08538778   
   the 136300th batch, loss is: 0.0831365    
   the 136350th batch, loss is: 0.09287409   
   the 136400th batch, loss is: 0.06749725   
   the 136450th batch, loss is: 0.07703883   
   the 136500th batch, loss is: 0.07740217   
   the 136550th batch, loss is: 0.06471124   
   the 136600th batch, loss is: 0.07869893   
   the 136650th batch, loss is: 0.07475699   
   the 136700th batch, loss is: 0.08029472   
   the 136750th batch, loss is: 0.06388891   
   the 136800th batch, loss is: 0.06252851   
   the 136850th batch, loss is: 0.07738863   
   the 136900th batch, loss is: 0.06482342   
   the 136950th batch, loss is: 0.08235522   
   the 137000th batch, loss is: 0.06666514   
   the 137050th batch, loss is: 0.07948055   
   the 137100th batch, loss is: 0.0917271    
   the 137150th batch, loss is: 0.07162672   
   the 137200th batch, loss is: 0.08019166   
   the 137250th batch, loss is: 0.077043     
   the 137300th batch, loss is: 0.09884632   
   the 137350th batch, loss is: 0.08194328   
   the 137400th batch, loss is: 0.0836475    
   the 137450th batch, loss is: 0.08365629   
   the 137500th batch, loss is: 0.08507973   
   the 137550th batch, loss is: 0.06717046   
   the 137600th batch, loss is: 0.09659121   
   the 137650th batch, loss is: 0.07700098   
   the 137700th batch, loss is: 0.07641841   
   the 137750th batch, loss is: 0.0845638    
('Epoch: ', 46, 'lr is: [0.001]')
   the 137800th batch, loss is: 0.06578101   
   the 137850th batch, loss is: 0.05574369   
   the 137900th batch, loss is: 0.0695238    
   the 137950th batch, loss is: 0.05710331   
   the 138000th batch, loss is: 0.07544558   
   the 138050th batch, loss is: 0.06891926   
   the 138100th batch, loss is: 0.06536941   
   the 138150th batch, loss is: 0.07006311   
   the 138200th batch, loss is: 0.07653268   
   the 138250th batch, loss is: 0.06780002   
   the 138300th batch, loss is: 0.07498495   
   the 138350th batch, loss is: 0.06663059   
   the 138400th batch, loss is: 0.07667264   
   the 138450th batch, loss is: 0.10079264   
   the 138500th batch, loss is: 0.08175426   
   the 138550th batch, loss is: 0.0966785    
   the 138600th batch, loss is: 0.07713308   
   the 138650th batch, loss is: 0.09000573   
   the 138700th batch, loss is: 0.07790996   
   the 138750th batch, loss is: 0.07091425   
   the 138800th batch, loss is: 0.06146834   
   the 138850th batch, loss is: 0.0806838    
   the 138900th batch, loss is: 0.07807703   
   the 138950th batch, loss is: 0.06253598   
   the 139000th batch, loss is: 0.08007732   
   the 139050th batch, loss is: 0.06150706   
   the 139100th batch, loss is: 0.05863543   
   the 139150th batch, loss is: 0.07878972   
   the 139200th batch, loss is: 0.08043491   
   the 139250th batch, loss is: 0.07424994   
   the 139300th batch, loss is: 0.08293272   
   the 139350th batch, loss is: 0.07316489   
   the 139400th batch, loss is: 0.073688     
   the 139450th batch, loss is: 0.06970073   
   the 139500th batch, loss is: 0.08399284   
   the 139550th batch, loss is: 0.06819371   
   the 139600th batch, loss is: 0.07238623   
   the 139650th batch, loss is: 0.08589862   
   the 139700th batch, loss is: 0.06565546   
   the 139750th batch, loss is: 0.08914624   
   the 139800th batch, loss is: 0.07070527   
   the 139850th batch, loss is: 0.0790562    
   the 139900th batch, loss is: 0.0721331    
   the 139950th batch, loss is: 0.0684276    
   the 140000th batch, loss is: 0.07318288   
   the 140050th batch, loss is: 0.08541419   
   the 140100th batch, loss is: 0.08601708   
   the 140150th batch, loss is: 0.06894819   
   the 140200th batch, loss is: 0.09027611   
   the 140250th batch, loss is: 0.07447595   
   the 140300th batch, loss is: 0.08592215   
   the 140350th batch, loss is: 0.08978087   
   the 140400th batch, loss is: 0.07796057   
   the 140450th batch, loss is: 0.10304054   
   the 140500th batch, loss is: 0.06348035   
   the 140550th batch, loss is: 0.05679609   
   the 140600th batch, loss is: 0.08248316   
   the 140650th batch, loss is: 0.07040246   
   the 140700th batch, loss is: 0.06278896   
   the 140750th batch, loss is: 0.08047506   
('Epoch: ', 47, 'lr is: [0.001]')
   the 140800th batch, loss is: 0.07259039   
   the 140850th batch, loss is: 0.07533115   
   the 140900th batch, loss is: 0.06952196   
   the 140950th batch, loss is: 0.07001232   
   the 141000th batch, loss is: 0.0844671    
   the 141050th batch, loss is: 0.08680924   
   the 141100th batch, loss is: 0.06152752   
   the 141150th batch, loss is: 0.06513807   
   the 141200th batch, loss is: 0.07601237   
   the 141250th batch, loss is: 0.07777094   
   the 141300th batch, loss is: 0.0620253    
   the 141350th batch, loss is: 0.07688571   
   the 141400th batch, loss is: 0.07599781   
   the 141450th batch, loss is: 0.059426     
   the 141500th batch, loss is: 0.06722587   
   the 141550th batch, loss is: 0.0655141    
   the 141600th batch, loss is: 0.05836223   
   the 141650th batch, loss is: 0.07127669   
   the 141700th batch, loss is: 0.07291694   
   the 141750th batch, loss is: 0.073871     
   the 141800th batch, loss is: 0.07215513   
   the 141850th batch, loss is: 0.06845952   
   the 141900th batch, loss is: 0.07195603   
   the 141950th batch, loss is: 0.08510516   
   the 142000th batch, loss is: 0.06952993   
   the 142050th batch, loss is: 0.07566004   
   the 142100th batch, loss is: 0.06400081   
   the 142150th batch, loss is: 0.06788015   
   the 142200th batch, loss is: 0.07624128   
   the 142250th batch, loss is: 0.07239174   
   the 142300th batch, loss is: 0.07581046   
   the 142350th batch, loss is: 0.08129063   
   the 142400th batch, loss is: 0.07225364   
   the 142450th batch, loss is: 0.05646867   
   the 142500th batch, loss is: 0.08477123   
   the 142550th batch, loss is: 0.07123931   
   the 142600th batch, loss is: 0.07141037   
   the 142650th batch, loss is: 0.0813406    
   the 142700th batch, loss is: 0.07420959   
   the 142750th batch, loss is: 0.0706749    
   the 142800th batch, loss is: 0.07051127   
   the 142850th batch, loss is: 0.08238631   
   the 142900th batch, loss is: 0.05948915   
   the 142950th batch, loss is: 0.09799708   
   the 143000th batch, loss is: 0.0700166    
   the 143050th batch, loss is: 0.0754731    
   the 143100th batch, loss is: 0.08889624   
   the 143150th batch, loss is: 0.07509556   
   the 143200th batch, loss is: 0.07341204   
   the 143250th batch, loss is: 0.08815472   
   the 143300th batch, loss is: 0.0746038    
   the 143350th batch, loss is: 0.08031639   
   the 143400th batch, loss is: 0.07339188   
   the 143450th batch, loss is: 0.11313174   
   the 143500th batch, loss is: 0.06928225   
   the 143550th batch, loss is: 0.08797438   
   the 143600th batch, loss is: 0.06769435   
   the 143650th batch, loss is: 0.08732259   
   the 143700th batch, loss is: 0.09673905   
   the 143750th batch, loss is: 0.09405769   
('Epoch: ', 48, 'lr is: [0.001]')
   the 143800th batch, loss is: 0.06481383   
   the 143850th batch, loss is: 0.10126843   
   the 143900th batch, loss is: 0.06316411   
   the 143950th batch, loss is: 0.07083654   
   the 144000th batch, loss is: 0.08435378   
   the 144050th batch, loss is: 0.07640323   
   the 144100th batch, loss is: 0.10666335   
   the 144150th batch, loss is: 0.07833892   
   the 144200th batch, loss is: 0.06365258   
   the 144250th batch, loss is: 0.06507386   
   the 144300th batch, loss is: 0.07282051   
   the 144350th batch, loss is: 0.07882632   
   the 144400th batch, loss is: 0.06206252   
   the 144450th batch, loss is: 0.06083309   
   the 144500th batch, loss is: 0.05741315   
   the 144550th batch, loss is: 0.06627634   
   the 144600th batch, loss is: 0.07268244   
   the 144650th batch, loss is: 0.09230992   
   the 144700th batch, loss is: 0.07427263   
   the 144750th batch, loss is: 0.07449859   
   the 144800th batch, loss is: 0.07245121   
   the 144850th batch, loss is: 0.08322676   
   the 144900th batch, loss is: 0.06916949   
   the 144950th batch, loss is: 0.07603455   
   the 145000th batch, loss is: 0.07666201   
   the 145050th batch, loss is: 0.06679326   
   the 145100th batch, loss is: 0.09702454   
   the 145150th batch, loss is: 0.09231156   
   the 145200th batch, loss is: 0.08350788   
   the 145250th batch, loss is: 0.06912668   
   the 145300th batch, loss is: 0.07545561   
   the 145350th batch, loss is: 0.06873495   
   the 145400th batch, loss is: 0.08087134   
   the 145450th batch, loss is: 0.05615924   
   the 145500th batch, loss is: 0.08493665   
   the 145550th batch, loss is: 0.0978899    
   the 145600th batch, loss is: 0.09719091   
   the 145650th batch, loss is: 0.07747576   
   the 145700th batch, loss is: 0.07805658   
   the 145750th batch, loss is: 0.08715568   
   the 145800th batch, loss is: 0.087703     
   the 145850th batch, loss is: 0.08536755   
   the 145900th batch, loss is: 0.08126719   
   the 145950th batch, loss is: 0.07119206   
   the 146000th batch, loss is: 0.08676428   
   the 146050th batch, loss is: 0.08999053   
   the 146100th batch, loss is: 0.09318676   
   the 146150th batch, loss is: 0.08140334   
   the 146200th batch, loss is: 0.0781376    
   the 146250th batch, loss is: 0.08158986   
   the 146300th batch, loss is: 0.07107352   
   the 146350th batch, loss is: 0.06746385   
   the 146400th batch, loss is: 0.07648782   
   the 146450th batch, loss is: 0.09762596   
   the 146500th batch, loss is: 0.07258119   
   the 146550th batch, loss is: 0.07180022   
   the 146600th batch, loss is: 0.07442947   
   the 146650th batch, loss is: 0.09658052   
   the 146700th batch, loss is: 0.07791356   
   the 146750th batch, loss is: 0.06935164   
('Epoch: ', 49, 'lr is: [0.001]')
   the 146800th batch, loss is: 0.07119398   
   the 146850th batch, loss is: 0.06539046   
   the 146900th batch, loss is: 0.07576463   
   the 146950th batch, loss is: 0.09140784   
   the 147000th batch, loss is: 0.05266657   
   the 147050th batch, loss is: 0.08956799   
   the 147100th batch, loss is: 0.06958942   
   the 147150th batch, loss is: 0.07752477   
   the 147200th batch, loss is: 0.06550364   
   the 147250th batch, loss is: 0.06572022   
   the 147300th batch, loss is: 0.06153019   
   the 147350th batch, loss is: 0.06378129   
   the 147400th batch, loss is: 0.07262614   
   the 147450th batch, loss is: 0.09005082   
   the 147500th batch, loss is: 0.06265338   
   the 147550th batch, loss is: 0.07194643   
   the 147600th batch, loss is: 0.09664544   
   the 147650th batch, loss is: 0.07623439   
   the 147700th batch, loss is: 0.08393273   
   the 147750th batch, loss is: 0.06908327   
   the 147800th batch, loss is: 0.06149359   
   the 147850th batch, loss is: 0.07030863   
   the 147900th batch, loss is: 0.06716708   
   the 147950th batch, loss is: 0.07271832   
   the 148000th batch, loss is: 0.06132621   
   the 148050th batch, loss is: 0.07132658   
   the 148100th batch, loss is: 0.06904899   
   the 148150th batch, loss is: 0.898377     
   the 148200th batch, loss is: 0.0678694    
   the 148250th batch, loss is: 0.07552416   
   the 148300th batch, loss is: 0.07670286   
   the 148350th batch, loss is: 0.07123595   
   the 148400th batch, loss is: 0.07937349   
   the 148450th batch, loss is: 0.07645284   
   the 148500th batch, loss is: 0.07908992   
   the 148550th batch, loss is: 0.08740811   
   the 148600th batch, loss is: 0.07157257   
   the 148650th batch, loss is: 0.0636116    
   the 148700th batch, loss is: 0.05800112   
   the 148750th batch, loss is: 0.09121136   
   the 148800th batch, loss is: 0.07761058   
   the 148850th batch, loss is: 0.06809421   
   the 148900th batch, loss is: 0.07944063   
   the 148950th batch, loss is: 0.08038373   
   the 149000th batch, loss is: 0.06003831   
   the 149050th batch, loss is: 0.06412336   
   the 149100th batch, loss is: 0.07416884   
   the 149150th batch, loss is: 0.07104111   
   the 149200th batch, loss is: 0.07082259   
   the 149250th batch, loss is: 0.07840374   
   the 149300th batch, loss is: 0.08390311   
   the 149350th batch, loss is: 0.08587623   
   the 149400th batch, loss is: 0.08296886   
   the 149450th batch, loss is: 0.0795548    
   the 149500th batch, loss is: 0.07270619   
   the 149550th batch, loss is: 0.07182069   
   the 149600th batch, loss is: 0.06454361   
   the 149650th batch, loss is: 0.07647178   
   the 149700th batch, loss is: 0.07612933   
Finished training.
